[
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "This is an example of embedding a widget with multiple calendars. This might be useful if you want to have different calendars for assignments, exams, instructor office hours, GSI office hours, etc.",
    "crumbs": [
      "Calendar"
    ]
  },
  {
    "objectID": "calendar.html#course-calendar",
    "href": "calendar.html#course-calendar",
    "title": "Calendar",
    "section": "",
    "text": "This is an example of embedding a widget with multiple calendars. This might be useful if you want to have different calendars for assignments, exams, instructor office hours, GSI office hours, etc.",
    "crumbs": [
      "Calendar"
    ]
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Course Staff",
    "section": "",
    "text": "[UNDER CONSTRUCTION]\nOffice hours are subject to change.\n\n\n\nInstructor: Shobhana Murali Stoyanov   shobhana@berkeley.edu Office hours:  - Mondays 10 am - 12 pm, Evans 333  pronouns: she/her\n\n\nGSI: Chuao Dong chuaodong@berkeley.edu   Office hours: TBD  pronouns: she/her\n\n\nGSI: [Thomas Lee])(#tl) tholee@berkeley.edu   Office hours: TBD  pronouns: he/him\n\n\nGSI: Tobias Roemer tobias_roemer@berkeley.edu   Office hours: TBD  pronouns: he/him\n\n\nTutor: Neha Suresh neha_suresh@berkeley.edu   pronouns: she/her\n\n\n\n\n\nReader: Jenny Gao    pronouns: she/her\n\n\nReader: Aurora Shi    pronouns: she/her",
    "crumbs": [
      "Staff"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "License"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "[UNDER CONSTRUCTION]",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "data.html#data-1",
    "href": "data.html#data-1",
    "title": "Data",
    "section": "Data 1",
    "text": "Data 1",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "data.html#data-2",
    "href": "data.html#data-2",
    "title": "Data",
    "section": "Data 2",
    "text": "Data 2",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "lectures/lecture-1.html",
    "href": "lectures/lecture-1.html",
    "title": "Syllabus, Course Overview, and Diagnostic Test",
    "section": "",
    "text": "The plan for today is to review the syllabus, read the overview of the course below, and work on the diagnostic test. Please give yourself about 30 minutes to complete it. Of course, if you take longer we will not know, but you should think about why it is taking you longer, and review those topics that you think you might have forgotten.\nOnce you are done with the test, please upload it to Gradescope. Since this does not count towards your course grade, it is due on Thursday night and there is no extension possible. If you turn it in, you get \\(0.25\\%\\) extra credit towards your final course average. If you don’t turn it in, it’s okay, these things happen. Don’t stress.",
    "crumbs": [
      "Lecture 1: Syllabus & Diagnostic Test"
    ]
  },
  {
    "objectID": "lectures/lecture-1.html#course-overview",
    "href": "lectures/lecture-1.html#course-overview",
    "title": "Syllabus, Course Overview, and Diagnostic Test",
    "section": "Course Overview",
    "text": "Course Overview\n\nProbability vs Statistics\n\n\n\n\n\nWhen we study probability, we have some data generating process (perhaps we are sampling from a known distribution, or we are sampling at random from a box with \\(N\\) items etc), and we want to know something about the random sample that we generate, and its properties.\nNow, when we study inference, we are given the sample, and we want to know more about the process that generated the data. That is, we want to infer something about the population, given the data that we observe.\n\n\nEstimation\n\nSimple Random Sampling\n\nWe might make no distributional assumptions about the population, and then try to estimate the population mean and variance and other parameters. This is the material we will study in Chapter 7, estimates from survey sampling. In this chapter we treat the estimate as a random variable that has a sampling distribution. This just means that each time we take a sample (of some fixed size) we get a different observed value of our random variable. All such possible values give us a distribution of the random variable (for example, the sample mean) which is called its sampling distribution.\nWhile investigating these estimates, we will talk about the concepts of bias, standard error, and mean squared error, confidence intervals, and how we apply the central limit theorem. Here are two visualizations of the CLT, with the top row showing the sampling distribution of the sample mean of random samples of \\(Unif(0,1)\\) random variables for various \\(n\\). The bottom row does the same for a \\(Gamma(2,1)\\) distribution. You can see how even though we begin with distributions that are very far from a normal distribution, for larger \\(n\\), the distribution of the sample mean is approximately normal!\n\n\n\n\n\n\n\n\n\n\n\nResampling methods: We will use the bootstrap to estimate confidence bounds for our parameters. When possible, we will compare these confidence intervals to the ones obtained by classical methods.\n\n\n\nEstimating distributional parameters\n\nOn the other hand, depending on what we know about the population, we might build models with distributional assumptions: for example, the number of goals in the soccer World Cup could be modeled using the Poisson distribution.\nOr perhaps we might consider classification problems, maybe whether an item is defective or not. In this case we might use the Bernoulli distribution.\nIf we assume some distribution, then we will want to estimate the parameters of that distribution. Chapter 8 is about parameter estimation, in which we have observed data, and try to fit probability laws to this data. We will learn about maximum likelihood estimation, and the method of moments, see which method is preferred and why. We will develop the ideas of Bayesian inference, and define an efficient estimator, and prove the famous Cramér-Rao inequality.\nWe will also derive properties of the distribution of our Maximum Likelihood Estimator \\(\\hat{\\theta}_{\\mathrm{mle}}\\), \\[\n\\hat{\\theta}_{\\mathrm{mle}} = \\operatorname*{arg\\,max}_{\\theta \\in \\Theta} \\hat{L}_n(\\theta;\\mathbf{y})\n\\]\nwhere \\[\n\\sqrt{n}(\\hat{\\theta}_{\\mathrm{mle}}-\\theta_0) \\rightarrow \\mathcal{N}(0, \\mathcal{I}^{-1})\n\\]\n\nand \\(\\mathcal{I}\\) is the Fisher information. All this is in Chapter 8 of our text, which is perhaps the most mathematical of the material we will cover.\n\n\n\nHypothesis Testing\n\nThis is the topic of Chapter 9. Hypothesis tests is a method of statistical inference in which we decide if the observed data supports a particular hypothesis. We will learn about the Neyman-Pearson paradigm. We also look at various tests, including goodness of fit, which we will also look at in Chapter 11. We will define the power of a test, and also look at the duality of hypothesis tests and confidence intervals.\nWe will further develop the ideas of hypothesis testing in Chapter 11, when we look at classical and nonparametric methods for two-sample tests, and in Chapters 12 and 13 we will see how ANOVA is used to compare multiple groups. Chapters 9 and 13 deal with categorical data, so we will look at these together.\nWe will also look at more computationally intensive resampling methods such as permutation tests.\n\n\n\nLinear Models\n\nChapter 14 is about linear least squares. We will go over the simple regression model in some detail, but only briefly cover the more general treatment.\n\n\n\nThe Bayesian Paradigm\n\nMost of the time, we will use a classical, frequentist approach to our analysis, in which parameters are fixed quantities, and probability statements are made only about sampled data and estimates.\nIn Bayesian statistics, the parameters are random, and we assume a distribution that reflects our beliefs about them. Then we update our beliefs given the observed data.\nWe compute posterior distributions (post-data) using Bayes’ Rule.\n\n\n\n\nImage from Nieves “An Actual Introduction to Bayesian Statistics (2021)”, cantorsparadise.com\n\n\n\n(Rice 2006; Wasserman 2004; Pimentel 2024; Blitzstein and Hwang 2019)",
    "crumbs": [
      "Lecture 1: Syllabus & Diagnostic Test"
    ]
  },
  {
    "objectID": "lectures/lecture-1.html#references",
    "href": "lectures/lecture-1.html#references",
    "title": "Syllabus, Course Overview, and Diagnostic Test",
    "section": "References",
    "text": "References\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. CRC Press.\n\n\nPimentel, Sam. 2024. “STAT 135 Lecture Slides.” Lecture slides (shared privately).\n\n\nRice, John A. 2006. Mathematical Statistics and Data Analysis. 3rd ed. Duxbury Press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. New York: Springer.",
    "crumbs": [
      "Lecture 1: Syllabus & Diagnostic Test"
    ]
  },
  {
    "objectID": "lectures/lecture-2.html",
    "href": "lectures/lecture-2.html",
    "title": "Survey Sampling",
    "section": "",
    "text": "In this chapter, we look at the topic of survey sampling, which involves a particular type of inference, saying something about a population, given an observed subset of the population. By now, we are all very used to sample surveys, such as presidential approval polls, and polls on various issues such as: What percentage of Republicans support vaccine requirements for children to attend public schools. Pew Research investigated this question a couple of years ago, which they attempted to answer by taking a sample of Republican voters, and then drawing a conclusion about the population of Republican voters.\n\n\n\n\n\nNews outlets are constantly publishing polls, which are certainly not all the same quality. The famous FiveThirtyEight site, started by Nate Silver, is defunct now, but it was famous for its pollster ratings. You can read about their methodology and here are their rankings from a couple of years ago.\n\n\n\nFiveThirtyEight pollster ratings from January 2024\n\n\nIn our course, we learn a little bit about survey sampling, and if you like it, you might think about taking Stat 152 the next time it is offered in our department. Chapter 7 in our text discusses the probabilistic sampling techniques, in that each population unit has a specified probability of being included in the sample, which consists of randomly selected units from the population. Note that we make no distributional assumptions in this chapter. We will restrict our study to Simple Random Samples: every population unit has the same probability of being selected, and each particular sample of size \\(n\\) has the same probability. That is, if the size of our population is \\(N\\), then each of the \\(\\displaystyle \\binom{N}{n}\\) possible samples of size \\(n\\) taken without replacement has the same probability.\n\n\nConsider the following problem:\nYou have a box containing 5 cards. Four of the cards are labeled with the number \\(0\\) and one of them is labeled with the number \\(1\\).You pick two cards at random with replacment. Let \\(Y\\) represent the average of the two cards.\n\nWhat is the distribution of the random variable \\(Y\\)? (Hint: Define \\(X\\) to be the sum of the two cards. What is the distribution of \\(X\\)?)\n\n\n\nCheck your answer\n\n\\(X \\sim Bin(2, \\dfrac{1}{5})\\), and \\(Y = X/2\\).\n\\(P(Y = 0) = P(X = 0) = \\dfrac{16}{25}\\)\n\\(P(Y = \\dfrac{1}{2}) = P(X = 1) = \\dfrac{8}{25}\\)\n\\(P(Y = 1) = P(X = 2) = \\dfrac{1}{25}\\)\n\n\nCompute \\(E(Y)\\) and \\(\\mathrm{Var}(Y)\\).\n\n\n\nCheck your answer\n\n\\(E(Y) = \\dfrac{1}{5}\\) and \\(\\mathrm{Var}(Y) = \\dfrac{2}{25}\\).\n\nNow what if I sample without replacement? Let \\(Z\\) be the average of the two tickets in this case. What is the distribution of \\(Z\\)?\n\n\nCheck your answer\n\nNow the sum is Hypergeometric. (What are the parameters of the distribution?)\nYou can work out that \\(P(Z = 0) = \\dfrac{3}{5}, P(Z = \\dfrac{1}{2}) = \\dfrac{2}{5}\\).\nWhy can’t \\(Z\\) be \\(1\\)?",
    "crumbs": [
      "Lecture 2: Survey Sampling, Introduction"
    ]
  },
  {
    "objectID": "lectures/lecture-2.html#introduction",
    "href": "lectures/lecture-2.html#introduction",
    "title": "Survey Sampling",
    "section": "",
    "text": "In this chapter, we look at the topic of survey sampling, which involves a particular type of inference, saying something about a population, given an observed subset of the population. By now, we are all very used to sample surveys, such as presidential approval polls, and polls on various issues such as: What percentage of Republicans support vaccine requirements for children to attend public schools. Pew Research investigated this question a couple of years ago, which they attempted to answer by taking a sample of Republican voters, and then drawing a conclusion about the population of Republican voters.\n\n\n\n\n\nNews outlets are constantly publishing polls, which are certainly not all the same quality. The famous FiveThirtyEight site, started by Nate Silver, is defunct now, but it was famous for its pollster ratings. You can read about their methodology and here are their rankings from a couple of years ago.\n\n\n\nFiveThirtyEight pollster ratings from January 2024\n\n\nIn our course, we learn a little bit about survey sampling, and if you like it, you might think about taking Stat 152 the next time it is offered in our department. Chapter 7 in our text discusses the probabilistic sampling techniques, in that each population unit has a specified probability of being included in the sample, which consists of randomly selected units from the population. Note that we make no distributional assumptions in this chapter. We will restrict our study to Simple Random Samples: every population unit has the same probability of being selected, and each particular sample of size \\(n\\) has the same probability. That is, if the size of our population is \\(N\\), then each of the \\(\\displaystyle \\binom{N}{n}\\) possible samples of size \\(n\\) taken without replacement has the same probability.\n\n\nConsider the following problem:\nYou have a box containing 5 cards. Four of the cards are labeled with the number \\(0\\) and one of them is labeled with the number \\(1\\).You pick two cards at random with replacment. Let \\(Y\\) represent the average of the two cards.\n\nWhat is the distribution of the random variable \\(Y\\)? (Hint: Define \\(X\\) to be the sum of the two cards. What is the distribution of \\(X\\)?)\n\n\n\nCheck your answer\n\n\\(X \\sim Bin(2, \\dfrac{1}{5})\\), and \\(Y = X/2\\).\n\\(P(Y = 0) = P(X = 0) = \\dfrac{16}{25}\\)\n\\(P(Y = \\dfrac{1}{2}) = P(X = 1) = \\dfrac{8}{25}\\)\n\\(P(Y = 1) = P(X = 2) = \\dfrac{1}{25}\\)\n\n\nCompute \\(E(Y)\\) and \\(\\mathrm{Var}(Y)\\).\n\n\n\nCheck your answer\n\n\\(E(Y) = \\dfrac{1}{5}\\) and \\(\\mathrm{Var}(Y) = \\dfrac{2}{25}\\).\n\nNow what if I sample without replacement? Let \\(Z\\) be the average of the two tickets in this case. What is the distribution of \\(Z\\)?\n\n\nCheck your answer\n\nNow the sum is Hypergeometric. (What are the parameters of the distribution?)\nYou can work out that \\(P(Z = 0) = \\dfrac{3}{5}, P(Z = \\dfrac{1}{2}) = \\dfrac{2}{5}\\).\nWhy can’t \\(Z\\) be \\(1\\)?",
    "crumbs": [
      "Lecture 2: Survey Sampling, Introduction"
    ]
  },
  {
    "objectID": "lectures/lecture-2.html#definitions-and-vocabulary",
    "href": "lectures/lecture-2.html#definitions-and-vocabulary",
    "title": "Survey Sampling",
    "section": "Definitions and vocabulary",
    "text": "Definitions and vocabulary\nThe figure below, adapted from Lohr’s book on sampling, shows that we have to be careful regarding the scope of our conclusions. We can only generalize from results from our sample to the sampled population, even if the target population (the population we are interested in) is something different!\n\n\n\n\n\n\nPopulation: the complete set of individuals or entities that we are interested in. We usually only have data on a subset of them (a sample). We will assume that our population is of (finite) size \\(N\\), and that associated with each member or unit of the population is some numerical value. We will denote these numbers by \\(x_1, x_2, \\ldots, x_N\\). If the values of the \\(x_i\\) are \\(0\\) or \\(1\\) then we are usually investigating the presence or absence of some characteristic, such as a particular party affiliation. In this case, our population is dichotomous or binary.\n\n\nParameter: any quantifiable feature of a population. For now, we will assume that the parameter is fixed but unknown.\nFor example: The mean age of all undergraduate students at UC Berkeley.\n\nThe most common population parameters that we are interested in are:\n\nPopulation mean or average: this is denoted by \\(\\mu\\) and defined to be: \\[\n\\mu = \\dfrac{1}{N}\\sum_{i = 1}^N x_i\n\\]\n\n\nPopulation proportion: This is just the population mean in the binary case, and we represent this special mean by \\(p\\) rather than \\(\\mu\\).\n\nOther parameters that we will consider:\n\nPopulation total: this is denoted by \\(\\tau\\) and defined to be: \\[\n\\tau = \\sum_{i = 1}^N x_i = N\\mu\n\\] Note that for a binary population, \\(\\tau\\) represents how many population units possess the characteristic of interest.\n\n\nPopulation variance: this is denoted by \\(\\sigma^2\\) and defined to be: \\[\n\\sigma^2 = \\dfrac{1}{N}\\sum_{i = 1}^N (x_i-\\mu)^2\n\\] The population standard deviation is the square root of the population variance.\n\nExercise: Show that \\(\\sigma^2\\) reduces to \\(\\displaystyle \\dfrac{1}{N}\\sum_{i = 1}^N x_i^2 -\\mu^2\\), and if the \\(x_i\\) are \\(0\\) or \\(1\\) only, then \\(\\sigma^2 = p(1-p)\\).\nExercise Going back to the results of the Pew Research survey shown at the beginning of these notes. What is the population and the parameter of interest?\n\n\nCheck your answer\n\nPopulation: US adults\nParameter: Percentage of US adults that think healthy children should be required to be vaccinated in order to attend public schools.\n\nExercise Consider a population of size 4: \\(\\{x_1, x_2, x_3, x_4\\}\\).\n\nIf we use simple random sampling, how many samples of size 2 will we have? What would be the expected value of the sample mean? Is it equal to the population mean?\nIf, rather than a simple random sample, when all samples of size 2 are equally likely, we use a different probabilistic scheme for getting our samples of size 2: the following four samples are equally likely, and only these samples are possible: \\(\\{x_1, x_2\\}, \\{x_2, x_3\\}, \\{x_3, x_4\\}, \\{x_1, x_4\\}\\). What would be the expected value of the sample mean? Is it equal to the population mean?",
    "crumbs": [
      "Lecture 2: Survey Sampling, Introduction"
    ]
  },
  {
    "objectID": "lectures/lecture-2.html#inference-in-sampling",
    "href": "lectures/lecture-2.html#inference-in-sampling",
    "title": "Survey Sampling",
    "section": "Inference in Sampling",
    "text": "Inference in Sampling\nInference involves using a sample to compute an estimate of a population parameter, and the population should always be defined in the context in which the results will be applied.\n\nEstimator: The function (or algorithm) that maps sample data to a number.\n\n\nEstimate: The actual observed value after applying the estimator on the sample (observed) data.\n\n\n\n\n\n\nThen of course the question we would have is how good is our estimator and therefore our estimate? We want \\(\\mu\\), and we have the estimate \\(\\hat{\\mu}\\). How close is this estimate to the true value \\(\\mu\\)? We need a measure of goodness of our estimator. Note that our estimator is random. Each time we take a random sample, we will get a different value of the estimate. We want to know on average, what is the error of our estimator? To compute this, we need to consider the sampling distribution of our estimator. This is just a special name for the probability distribution of the estimator, which is a random variable. The randomness of the estimator is rooted in the randomness of the sampling. The spread of the probability distribution, measured by its standard deviation is one of the determinants of the accuracy of our estimator.\n\n\n\n\n\nIf we would hit our target (the population parameter) on average (that means that the expected value of our estimator is the population parameter), then we only need to consider how much our estimator’s sampling distribution spreads about the mean. The tighter the spread (the smaller the standard deviation), the more accurate the estimator. Now, because we are measuring the error of our estimator, we call the square root of its variance the standard error rather than the standard deviation.\nWhat if, though, the expected value of our estimator is not the target parameter? In this case the difference between the expected value of the estimator and the true value of the population parameter will also contribute to the error. Because of this, we use a measure of goodness of our estimate that incorporates both the spread (standard error) and the average distance from the parameter (we call this bias). This measure is called the Mean Squared Error.\n\nMean Squared Error\n\nMean Squared Error: The mean squared error is the expected value of the squared difference between the estimator \\(\\hat{\\theta}\\) and the true value of the population parameter \\(\\theta\\). We denoted it by \\(MSE\\): \\[\nMSE = E\\left[\\left(\\hat{\\theta} - \\theta \\right)^2\\right]\n\\]\n\n\nBias: The bias of an estimator is its distance, on average, from the true value of the population parameter: \\[\n\\operatorname{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\n\\] We call an estimator unbiased if the bias is 0, that is if \\(E(\\hat{\\theta}) = \\theta\\).\n\nExercise Show that \\(MSE =  \\text{Variance} + \\text{Bias}^2\\).\n\n\nSolution\n\n\\[\n\\begin{align*}\n\\mathrm{MSE}(\\hat\\theta)\n&= E\\big[(\\hat\\theta - \\theta)^2\\big] \\\\\n&= E\\big(\\hat\\theta ^2\\big) -2 \\theta E\\big(\\hat\\theta\\big) + \\theta^2\\\\\n&= \\mathrm{Var}\\big(\\hat\\theta\\big) +  \\big[E\\big(\\hat\\theta\\big)\\big]^2 -2 \\theta E\\big(\\hat\\theta\\big) + \\theta^2\\\\\n&= \\mathrm{Var}\\big(\\hat\\theta\\big) + \\big[E\\big(\\hat\\theta\\big) - \\theta\\big]^2\\\\\n&= \\mathrm{Var}(\\hat\\theta) + \\big[\\mathrm{Bias}(\\hat\\theta)\\big]^2.\n\\end{align*}\n\\]\n\nHere is a figure from Lohr’s text that shows the difference between low bias, low variance, and low MSE:\n\n\n\nUnbiased archers, precise archers, and accurate archers\n\n\nThis diagram shows that an estimator \\(\\hat{\\theta}\\) is unbiased if \\(E\\big(\\hat{\\theta}\\big) = \\theta\\), it is precise if \\(\\mathrm{Var}(\\hat{\\theta})\\) is small, but for the estimator to be accurate, both these quantities must be small, and therefore the Mean Squared Error (the sum of the squared bias and the variance) must be small, where \\(MSE = E\\big[\\big(\\hat{\\theta} - \\theta\\big)^2\\big]\\).\n\n\n\\(\\mathrm{Var}(\\overline{X})\\) and the finite population correction\nRecall that if we let \\(X_1, X_2, \\ldots, X_n\\) are independent and identically distributed random variables (IID), with common expected value \\(\\mu\\) and variance \\(\\sigma^2\\); and \\(\\overline{X}\\) is the sample mean of this sample \\(\\big(\\displaystyle \\overline{X} = \\dfrac{1}{n} \\sum_{i=1}^n X_i\\big)\\), then \\(E(\\overline{X}) = \\mu\\) and \\(\\mathrm{Var}(\\overline{X}) = \\sigma^2/n\\). (Note: You should be able to show this.)\nNow suppose we have a finite population of size \\(N\\), and we take a simple random sample of size \\(n\\) from this population: \\(X_1, X_2, \\ldots, X_n\\). Now the \\(X_i\\) cannot be IID as we are sampling without replacement. It is easily shown (Theorem A on page 206) that the expected value of the sample mean is still \\(\\mu\\), where \\(\\mu\\) is the population mean. What about \\(\\mathrm{Var}(\\overline{X})\\)?\nIt turns out that ( Theorem B on page 208): \\[\n\\mathrm{Var}(\\overline{X}) = \\dfrac{\\sigma^2}{n}\\left( \\dfrac{N-n}{N-1}\\right).\n\\]\n\n\nClick for the proof\n\n\\[\n\\begin{align*}\n\\mathrm{Var}(\\overline{X}) &= \\mathrm{Var}\\left(\\dfrac{1}{n}\\sum_{i = 1}^n X_i\\right)\\\\\n  &= \\dfrac{1}{n^2} \\mathrm{Var}\\left(\\sum_{i = 1}^n X_i\\right), \\: \\text{because }\\mathrm{Var}(aX) = a^2\\mathrm{Var}(X)\\\\\n  &= \\dfrac{1}{n^2}\\mathrm{Cov}\\left(\\sum_{i = 1}^n X_i,\\sum_{j = 1}^n X_j \\right), \\: \\text{because } \\mathrm{Var}(X) = \\mathrm{Cov}(X,X)\\\\\n  &= \\dfrac{1}{n^2}\\left(\\sum_{i = 1}^n \\mathrm{Var}(X_i) + \\sum_{i = 1}^n \\sum_{\\substack{j=1 \\\\ j \\ne i}}^n \\mathrm{Cov}(X_i,X_j)\\right) \\\\\n  &= \\dfrac{1}{n^2} \\left( n\\sigma^2 + n(n-1) \\mathrm{Cov}(X_1, X_2) \\right), \\: \\text{since all } n(n-1) \\text{ pairs will have the same covariance.}\n\\end{align*}\n\\] This computation implies that if we figure out \\(\\mathrm{Cov}(X_1, X_2)\\), we will able to figure out the variance we need. So let’s compute this covariance. Recall that \\(\\mathrm{Cov}(X_1, X_2) = E(X_1 X_2) - E(X_1)E(X_2)\\).\nWe know that the possible values of the \\(X_i\\) are the population values: \\(x_1, x_2, \\ldots, x_N\\). But some of these could be repeated, which can mess up the probability computations. To simplify our computations, we will define new values \\(u_1, u_2, \\ldots, u_m\\) to be the distinct values in the population, \\(m\\) the number of distinct values, and let \\(n_i\\) be the number of times we see the value \\(u_i\\).\nFor example, suppose \\(N = 6\\) and the population values are \\(1, 1, 4, 4, 4, 7\\). Then \\(x_1 = 1 = x_2, x_3 = x_4 = x_5 = 4\\), and \\(x_6 = 7\\). Using the \\(u_i's\\), we have \\(u_1 = 1, u_2 = 4, u_3 = 7\\), and \\(m=3\\). Further, \\(n_1 = 2, n_2 = 3, n_3 = 1\\).\nNow, if \\(X_i\\) is the \\(i\\)th sample value drawn, then \\(X_i\\) is a discrete random variable such that \\(P(X_i = u_i) = \\dfrac{n_i}{N}\\). This is because there are still \\(N\\) total units in the population, and we have just grouped them by value.\nFor example, using the numbers above, \\(P(X_i = 4) = \\dfrac{3}{6}\\).\nYou can check that \\(E(X_i) = \\mu\\) and \\(\\mathrm{Var}(X_i) = \\sigma^2\\), using the fact that \\(\\displaystyle \\sum_{j=1}^m u_j n_j =  \\sum_{i=1}^N x_i\\).\nNow let’s compute \\(\\mathrm{Cov}(X_1, X_2) = E(X_1 X_2) - \\mu^2\\).\n\\[\n\\begin{align*}\nE(X_1 X_2) &= \\sum_{i=1}^m \\sum_{j=1}^m u_i u_j P(X_1 = u_i, X_2 = u_j) \\\\\n    &= \\sum_{i=1}^m \\sum_{j=1}^m u_i u_j P(X_1 = u_i) P(X_2 = u_j \\vert X_1 = u_i) \\\\\n    &= \\sum_{i=1}^m u_i P(X_1 = u_i) \\sum_{j=1}^m u_j P(X_2 = u_j \\vert X_1 = u_i)\\\\\n\\end{align*}\n\\] Now, as we discussed earlier, \\(P(X_1 = u_i) = \\dfrac{n_i}{N}\\). But the second draw from the population, \\(X_2\\) will depend on the first. \\(P(X_2 = u_j \\vert X_1 = u_i) = \\dfrac{n_j}{N-1}\\) if \\(j \\ne i\\) and \\(P(X_2 = u_j \\vert X_1 = u_i) = \\dfrac{n_i-1}{N-1}\\) if \\(j = i\\).\nThus, we can simplify the interior sum to: \\[\n\\begin{align*}\n\\sum_{j=1}^m u_j P(X_2 = u_j \\vert X_1 = u_i) &= \\sum_{\\substack{j=1 \\\\ j \\ne i}}^m u_j \\cdot \\dfrac{n_j}{N-1} + u_i\\cdot \\dfrac{n_i-1}{N-1}\\\\\n  &= \\sum_{\\substack{j=1 \\\\ j \\ne i}}^m u_j \\cdot \\dfrac{n_j}{N-1} + u_i\\cdot \\dfrac{n_i}{N-1} - u_i\\cdot \\dfrac{1}{N-1}\\\\\n  &= \\sum_{j=1}^m  \\dfrac{u_j n_j}{N-1} - \\dfrac{u_i}{N-1}\n\\end{align*}  \n\\] Back to \\(E(X_1 X_2)\\), noting that \\(\\displaystyle \\sum_{i=1}^m u_i n_i = \\sum_{k=1}^N x_k = \\tau = N\\mu\\), that is, the sum total of all the population values, and also note that \\(\\displaystyle \\sum_{i=1}^m u_i^2 n_i = \\sum_{i=1}^N x_i^2 = N(\\sigma^2 + \\mu^2)\\):\n\\[\n\\begin{align*}\nE(X_1 X_2) &= \\sum_{i=1}^m u_i \\dfrac{n_i}{N}\\left[ \\sum_{j=1}^m  \\dfrac{u_j n_j}{N-1} - \\dfrac{u_i}{N-1}\\right]\\\\\n   &= \\dfrac{1}{N(N-1)}\\left[ \\left( \\sum_{i=1}^m u_i n_i\\right)\\left( \\sum_{j=1}^m u_j n_j\\right) - \\sum_{i=1}^m u_i^2 n_i\\right]\\\\\n   &= \\dfrac{1}{N(N-1)} \\left[ \\left(N\\mu\\right)^2 -\\sum_{i=1}^m u_i^2 n_i\\right]\\\\\n   &= \\dfrac{1}{N(N-1)}\\left(N^2 \\mu^2 - N(\\sigma^2 + \\mu^2) \\right)\\\\\n   &= \\mu^2 -\\dfrac{\\sigma^2}{N-1}\n\\end{align*}\n\\] This implies that: \\[\n\\begin{align*}\n\\mathrm{Cov}(X_1, X_2) &= E(X_1X_2) - \\mu^2\\\\\n   &= \\mu^2 -\\dfrac{\\sigma^2}{N-1} - \\mu^2\\\\\n   &= -\\dfrac{\\sigma^2}{N-1}\n\\end{align*}\n\\]\nNow we can put it all together: \\[\n\\begin{align*}\n\\mathrm{Var}(\\overline{X}) &= \\dfrac{1}{n^2} \\left( n\\sigma^2 + n(n-1) \\mathrm{Cov}(X_1, X_2) \\right)\\\\\n   &= \\dfrac{1}{n^2} \\left( n\\sigma^2 - n(n-1)\\dfrac{\\sigma^2}{N-1}\\right)\\\\\n   &= \\dfrac{\\sigma^2}{n}\\left(1- \\dfrac{n-1}{N-1}\\right)\\\\\n   &= \\dfrac{\\sigma^2}{n}\\left(\\dfrac{N-n}{N-1}\\right)\n\\end{align*}\n\\]\n\n\nFinite population correction\nThe quantity \\(\\displaystyle \\left( \\dfrac{N-n}{N-1}\\right)=\\left(1- \\dfrac{n-1}{N-1}\\right)\\) is called the finite population correction. Note that \\(\\displaystyle \\dfrac{n-1}{N-1} \\approx \\dfrac{n}{N}\\), which is called the sampling fraction. The larger the sampling fraction, the larger the sample relative to the population, which means we have more information about the population. This should reduce the variability. The extreme case is when \\(n=N\\), and the sample mean has no variability. In practice, the sampling fraction is very small, and so the finite population correction is approximately 1. This means that the precision of the estimator (determined by the variance) depends only on the sample size, and not on the population size.\n\n\n\nEstimating the Population Variance\nWe know that the population variance \\(\\sigma^2\\) is defined by: \\[\n\\sigma^2 = \\dfrac{1}{N}\\sum_{i=1}^N x_i^2 - \\mu^2.\n\\] We can define the quantity \\(\\hat{\\sigma}^2\\), which is a function of the sample \\(X_1, X_2, \\ldots, X_n\\): \\[\n\\begin{align*}\n\\hat{\\sigma}^2 &= \\dfrac{1}{n} \\sum_{i=1}^n (X_i -\\overline{X})^2 \\\\\n    &= \\dfrac{1}{n} \\sum_{i=1}^n X_i^2 -\\overline{X}^2 \\\\\n\\end{align*}\n\\] and use this to estimate \\(\\sigma^2\\). The question is then if this estimator is unbiased. Is \\(E(\\hat{\\sigma}^2) = \\sigma^2\\)?\n\\[\n\\begin{align*}\nE(\\hat{\\sigma}^2) &= E\\left( \\dfrac{1}{n} \\sum_{i=1}^n X_i^2 -\\overline{X}^2 \\right) \\\\\n   &= \\dfrac{1}{n} \\sum_{i=1}^n E\\left(X_i^2\\right) - E\\big(\\overline{X}^2 \\big)\\\\\n   &=  (\\sigma^2 + \\mu^2) - E\\big(\\overline{X}^2 \\big)\\\\\n\\end{align*}\n\\] The last line is because \\(\\mathrm{Var}(X_i) = \\sigma^2 = E(X_i^2) - \\mu^2\\). Doing a similar computation with \\(E\\big(\\overline{X}^2 \\big)\\), we see that (for a simple random sample): \\[\nE\\big(\\overline{X}^2 \\big) = \\mathrm{Var}(\\overline{X}) + \\mu^2 = \\dfrac{\\sigma^2}{n}\\left(\\dfrac{N-n}{N-1} \\right) + \\mu^2.\n\\] Putting these together, and doing some tedious algebra offline, we have \\[\n\\begin{align*}\nE(\\hat{\\sigma}^2) &= (\\sigma^2 + \\mu^2) - \\left[\\frac{\\sigma^2}{n}\\left(\\dfrac{N-n}{N-1} \\right) + \\mu^2\\right] \\\\\n   &= \\dfrac{n\\sigma^2}{n} - \\frac{\\sigma^2}{n}\\left(\\dfrac{N-n}{N-1} \\right) + \\mu^2 - \\mu^2  \\\\\n   &= \\frac{\\sigma^2}{n} \\left[n - \\left(\\dfrac{N-n}{N-1} \\right)\\right] \\\\\n   &= \\sigma^2 \\left[\\left(\\frac{n-1}{n}\\right) \\left(\\dfrac{N}{N-1} \\right)\\right] \\\\\n   &= \\frac{\\sigma^2}{n} \\left[\\dfrac{nN-N}{nN-1}\\right]\\\\\n\\end{align*}\n\\] This means that \\(E(\\hat{\\sigma}^2) \\ne \\sigma^2\\), and also that \\(\\hat{\\sigma}^2\\) underestimates \\(\\sigma^2\\) on average (since \\(N &gt; 1\\)). Therefore, to get an unbiased estimator of the variance of the sample mean, we need to multiply \\(\\hat{\\sigma}^2\\) by the appropriate factor. Note that: \\[\nE\\left[\\left(\\frac{n}{n-1}\\right) \\left(\\dfrac{N-1}{N} \\right)\\hat{\\sigma}^2\\right] = \\sigma^2\n\\] Our goal, of course, is to get an unbiased estimator for the variance of the sample mean. Recall that, for a simple random sample, \\(Var(\\overline{X}) = \\dfrac{\\sigma^2}{n}\\left(\\dfrac{N-n}{N-1}\\right)\\). Let’s substiute the unbiased estimator that we just derived above: \\[\n\\begin{align*}\n\\mathrm{(Estimated)\\, Var}(\\overline{X}) &= \\left(\\frac{n}{n-1}\\right) \\left(\\dfrac{N-1}{N} \\right)\\hat{\\sigma}^2 \\cdot \\frac{1}{n} \\left(\\dfrac{N-n}{N-1}\\right) \\\\\n&= \\frac{\\hat{\\sigma}^2}{n-1}\\left(\\frac{N-n}{N}\\right)\\\\\n&= \\frac{s^2}{n}\\left(1-\\frac{n}{N}\\right)\n\\end{align*}\n\\] where \\(\\displaystyle s^2 = \\dfrac{1}{n-1} \\sum_{i=1}^n \\big(X_i - \\overline{X}\\big)^2\\), so that \\(\\displaystyle \\dfrac{s^2}{n} = \\dfrac{\\hat{\\sigma}^2}{n-1}\\).\nThus we have that \\(s_{\\overline{X}}^2\\) is an unbiased estimator of \\(\\sigma_{\\overline{X}}^2=\\mathrm{Var}(\\overline{X})\\).\nIf the population is dichotomous, then the estimator becomes: \\[\n\\mathrm{Var}(\\hat{p}) = s_{\\hat{p}}^2 = \\frac{s^2}{n} = \\frac{\\hat{p}(1-\\hat{p})}{n-1}.\n\\]\nPutting all this together gives us the table on page 214 of the text, reproduced here:\n\nSummary of estimators\n\n\n\n\n\n\n\n\n\n\nPopulation Parameter\nEstimator\nVariance of Estimator (Square of Standard Error)\nEstimated Variance of the Estimator  (Square of Estimated SE)\n\n\n\n\n\\(\\mu\\)\n\\(\\overline{X}\\)\n\\(\\sigma_{\\overline{X}}^2 = \\displaystyle \\dfrac{\\sigma^2}{n}\\left(\\dfrac{N-n}{N-1}\\right)\\)\n\\(s_{\\overline{X}}^2  = \\displaystyle \\dfrac{s^2}{n}\\left(1-\\dfrac{n}{N}\\right)\\)\n\n\n\\(p\\)\n\\(\\hat{p}\\)\n\\(\\sigma_{\\hat{p}}^2 = \\displaystyle \\dfrac{p(1-p)}{n}\\left(\\dfrac{N-n}{N-1}\\right)\\)\n\\(s_{\\hat{p}}^2  = \\displaystyle \\dfrac{\\hat{p}(1-\\hat{p})}{n-1}\\left(1-\\dfrac{n}{N}\\right)\\)\n\n\n\\(\\tau\\)\n\\(T = N\\overline{X}\\)\n\\(\\sigma_{\\tau}^2 = N^2\\sigma_{\\overline{X}}^2\\)\n\\(s_{\\tau}^2 = N^2s_{\\overline{X}}^2\\)\n\n\n\\(\\sigma^2\\)\n\\(\\left(1-\\dfrac{1}{N}\\right)s^2\\)\n\n\n\n\n\n\n\n\n\nThe (Asymptotic) Sampling Distribution of the Sample Mean\n\nThe Central Limit Theorem\nThe CLT states that for large \\(n\\), the sample mean, suitably standardized, will have a CDF that approaches the CDF of the standard Normal. That is, the standardized sample mean converges in distribution to the standard Normal.\nIf \\(X_1, X_2, \\ldots, X_n\\) is an independent and identically distributed sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then:\n\\[\n\\left(\\frac{\\overline{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\right) = \\sqrt{n}\\left(\\frac{\\overline{X}-\\mu}{\\sigma}\\right) \\overset{dsn}{\\longrightarrow}\\mathcal{N}(0,1)  \\text{ as } n\\longrightarrow \\infty\n\\] The CDF converges to \\(\\Phi\\), which means that: \\[\nP\\left(\\frac{\\overline{X}-\\mu}{\\dfrac{\\sigma}{\\sqrt{n}}} \\le z \\right) = F_{\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}}(z)\\longrightarrow \\Phi(z)\n\\]\nThe CLT is an incredibly important theorem, because it guarantees that for a large enough sample, no matter what the distribution of the random variables \\(X_i\\), the sample mean behaves as though it is from an approximately \\(\\mathcal{N}(\\mu, \\dfrac{\\sigma^2}{n})\\) distribution.\nNote that the CLT is a limit theorem, so it fully characterizes the asymptotic distribution of the sample mean. It provides a good approximation for large enough samples, so we can compute probabilities such as the \\(P\\)-value, and construct confidence intervals. We usually have a fixed population size \\(N\\), so it doesn’t make sense for the sample size \\(n \\rightarrow \\infty\\), but as long as \\(n\\) is large, but the sampling fraction \\(\\dfrac{n}{N}\\) is small, the normal approximation is pretty good, and how we use it is demonstrated in the following example.\n\n\nExample A, page 201\nThis is an example from the text that is used to illustrate many of the ideas from this chapter. Herkson (JASA 1976) presented data on the number of patients discharged from each of a population of \\(N=393\\) hospitals during January 1986. The mean number of discharges across the population is about 815, and the population standard deviation is about 590.\n\nIf a random sample of \\(n=100\\) is taken from this population with replacement, what is the standard error of the associated estimator of the population mean \\(\\overline{X}\\)?\nWhat is the standard error of \\(\\overline{X}\\) if instead we take a simple random sample of size \\(n=100\\)?\nIf we are sampling with replacement, what is the (approximate) probability that our sample average exceeds 850?\n\n\n\nCheck your answer\n\n\nSince we are sampling with replacement, the random variables form an IID sample, and so the standard error of the sample mean is \\(\\dfrac{\\sigma}{\\sqrt{n}} = \\dfrac{590}{\\sqrt{100}} = 59\\).\nIn the case of an SRS, the standard error of the sample mean is given by \\(\\dfrac{\\sigma}{\\sqrt{n}}\\left(\\dfrac{N-n}{N-1}\\right) = \\dfrac{590}{\\sqrt{100}}\\sqrt{\\left(\\dfrac{393-100}{393-1}\\right)} \\approx 51\\).\nWe want to approximate \\(P(\\overline{X} &gt; 850)\\). By the CLT, \\[\nP(\\overline{X} &gt; 850) = P\\left(\\dfrac{\\overline{X}-815}{59} &gt; \\dfrac{850-815}{59}\\right) = 1-\\Phi\\left(\\dfrac{850-815}{59}\\right) \\approx 0.2765.\n\\] We used 1-pnorm((850-815)/59) to compute the answer.\n\n\n\n\n\nConfidence intervals for the Population Mean\nAnother way we could use the central limit theorem with estimated standard error is to build a range of plausible values for the population mean from the sample. This range is called the confidence interval. A confidence interval  for some population parameter \\(\\theta\\) is a random interval, whose endpoints are constructed using the sample, such that the interval contains \\(\\theta\\) with some specified probability.\nIt is very important to note the assumption that the population parameter is fixed, and it is the interval that is random, and so the probability of coverage is associated with the random interval.\nSince we compute the endpoints using the random sample, the endpoints are random variables. This means that each time we take a sample of size \\(n\\), and then plug in our observed data, we will get a different realization of this random interval. But because we can use the CLT to approximate probabilities, we can construct intervals that have a probability of \\(1-\\alpha\\) of containing the true value. For example if \\(\\alpha = 0.05\\), we construct an interval that contains the true mean 95% of the times (on average).\nFor \\(0 \\le \\alpha \\le 1\\), let \\(z(\\alpha)\\) denote that value on the \\(x\\)-axis such that the area under the standard normal density curve to the right of \\(z(\\alpha)\\) is \\(\\alpha\\). When we have a particular confidence interval, it is a realization of a random interval, where the random interval has a certain coverage probability of \\(1-\\alpha\\). We call this coverage probability the confidence level.\n\n\n\n\n\n\n\n\n\nLet’s derive the confidence interval for the population mean \\(\\mu\\). By the central limit theorem, we know that \\(\\overline{X}\\) is approximately normal, that is, \\(\\dfrac{\\overline{X}-\\mu}{\\sigma_{\\overline{X}}} \\approx \\mathcal{N}(0,1)\\).\nIf \\(Z\\) follows the standard Normal distribution, then by the definition of \\(z(\\alpha)\\) above, we see that \\[\nP\\big(-z(\\alpha/2) \\le Z \\le z(\\alpha/2)\\big) = 1-\\alpha.\n\\] Therefore, if \\(\\dfrac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}\\) is approximately normal, we have that: \\[\nP\\left(-z(\\alpha/2) \\le \\dfrac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\le z(\\alpha/2)\\right) \\approx 1-\\alpha.\n\\] Now we multiply by \\(\\sigma/\\sqrt{n}\\), subtract \\(\\overline{X}\\) and multiply by \\(-1\\). This gives us the confidence interval that we need: \\[\nP\\left(\\overline{X} - \\frac{\\sigma}{\\sqrt{n}}z(\\alpha/2) \\le  \\mu \\le \\overline{X} +  \\frac{\\sigma}{\\sqrt{n}}z(\\alpha/2)\\right) \\approx 1-\\alpha.\n\\] This statement says that the chance of this random interval, \\(\\left(\\overline{X} - \\dfrac{\\sigma}{\\sqrt{n}}z(\\alpha/2) ,\\; \\overline{X} +  \\dfrac{\\sigma}{\\sqrt{n}}z(\\alpha/2)\\right)\\) capturing the mean is approximately \\(1-\\alpha\\), and so the interval is called a \\(100(1-\\alpha)%\\) confidence interval.\nWe can then plug in our observed value of \\(\\overline{X}\\) and will get a realization of the random interval: \\(\\left(\\overline{x} - \\dfrac{\\sigma}{\\sqrt{n}}z(\\alpha/2) ,\\; \\overline{x} +  \\dfrac{\\sigma}{\\sqrt{n}}z(\\alpha/2)\\right).\\) Note that this interval is not random. It is just an interval on the real line and as \\(\\mu\\) is just some fixed constant, it either lies in this interval or does not. Therefore, once we plug in the observed sample mean, and the observed value of the estimator \\(s_{\\overline{X}}\\), we don’t have any randomness. All the randomness is in the sampling procedure.\nWe can use the confidence interval to plan our data collection. Since the width of the confidence interval is given by \\(2\\times \\dfrac{\\sigma}{\\sqrt{n}} \\times z(\\alpha)\\), it is determined by \\(\\sigma\\) and by \\(\\sqrt{n}\\). Now \\(\\sigma\\) is a constant of the population, so we can’t do much with it, but we can choose \\(n\\) so that our confidence interval is as narrow as we desire. The margin of error of the confidence interval is given by $ z().$\nExercise (Problem 8 from section 7.7) A sample of size 100 is taken from a population that has a proportion \\(p = 1/5\\).\n\nFind \\(\\delta\\) such that \\(P\\big(\\lvert \\hat{p}-p\\rvert \\ge \\delta\\big)=0.025\\)\nIf, in the sample, \\(\\hat{p} = 0.25\\), will the 95% confidence interval for \\(p\\) contain the true value for \\(p\\)?\n\n\n\nCheck your answer\n\n\n\\(\\delta = 0.0896\\), \\(p\\) is given to be \\(\\dfrac{1}{5}\\). Therefore, \\(\\sigma_{\\hat{p}} = \\displaystyle \\sqrt{\\dfrac{\\frac{1}{5}\\cdot \\frac{4}{5}}{100}} = \\frac{2}{50}.\\)\n\nBy the Central Limit Theorem, \\(\\hat{p}\\) is approximately \\(\\mathcal{N}(p, \\sigma_{\\hat{p}}^2)\\).\n\\[\n\\begin{align*}\nP\\big(\\lvert \\hat{p}-p\\rvert \\ge \\delta\\big) &= 0.025 \\\\\n\\Rightarrow P\\big(\\lvert \\hat{p}-p\\rvert &lt; \\delta\\big) &= 0.975 \\\\\n\\Rightarrow P(-\\delta &lt; \\hat{p}-p &lt; \\delta)  &= 0.975 \\\\\n\\Rightarrow P\\left(-\\frac{\\delta}{\\sigma_{\\hat{p}}} &lt; \\frac{\\hat{p}-p}{\\sigma_{\\hat{p}}} &lt; \\frac{\\delta}{\\sigma_{\\hat{p}}}\\right)  &= 0.975 \\\\\n\\Rightarrow P\\left(-\\frac{\\delta}{\\sigma_{\\hat{p}}} &lt; Z &lt; \\frac{\\delta}{\\sigma_{\\hat{p}}}\\right)  &\\approx 0.975 \\\\\n\\Rightarrow \\Phi\\left(\\frac{\\delta}{\\sigma_{\\hat{p}}}\\right) - \\Phi\\left(-\\frac{\\delta}{\\sigma_{\\hat{p}}}\\right) &\\approx 0.975 \\\\\n\\Rightarrow 2\\Phi\\left(\\frac{\\delta}{\\sigma_{\\hat{p}}}\\right) -1 &\\approx 0.975 \\\\\n\\Rightarrow \\Phi\\left(\\frac{\\delta}{\\sigma_{\\hat{p}}}\\right) &\\approx 0.9875 \\\\\n\\Rightarrow \\frac{\\delta}{\\sigma_{\\hat{p}}} &\\approx 2.24\\\\\n\\end{align*}\n\\] Where we used qnorm(0.9875) to obtain 2.24. Plugging in the value of \\(\\sigma_{\\hat{p}} = \\dfrac{2}{50}\\), we get that \\(\\delta\\) is about \\(0.0896\\).\n\nYes.\n\n\\(z(\\alpha/2) = 1.96\\), and the 95% confidence interval is given by \\(\\hat{p} \\pm 1.96\\times \\dfrac{2}{50}\\). Since \\(\\hat{p} = 0.25\\), this gives us \\(0.25 \\pm 1.96\\times \\dfrac{2}{50} = (0.1716, 0.3284)\\) which contains \\(p = \\dfrac{1}{5}.\\)\n\nExercise 20 different polling companies have conducted independent surveys to estimate the proportion of US voters who approve of RFK Jr’s stewardship of Health and Human Services. Each company estimates this proportion using a 95% confidence interval. About how many do you think will be successful in covering the true proportion?\n\n\nCheck your answer\n\nIf we let \\(Y\\) be the number of confidence intervals out of 20 that are successful, then since each interval has a 0.95 chance of success, we see that \\(Y\\sim Bin(20, 0.95)\\). Therefore the expected number of successful intervals is \\(E(Y) = 20\\times 0.95 = 19.\\)\n\n\n(Rice 2006; Wasserman 2004; Pimentel 2024; Lohr 2010)",
    "crumbs": [
      "Lecture 2: Survey Sampling, Introduction"
    ]
  },
  {
    "objectID": "lectures/lecture-2.html#references",
    "href": "lectures/lecture-2.html#references",
    "title": "Survey Sampling",
    "section": "References",
    "text": "References\n\n\nLohr, Sharon L. 2010. Sampling: Design and Analysis. 2nd ed. Cengage.\n\n\nPimentel, Sam. 2024. “STAT 135 Lecture Slides.” Lecture slides (shared privately).\n\n\nRice, John A. 2006. Mathematical Statistics and Data Analysis. 3rd ed. Duxbury Press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. New York: Springer.",
    "crumbs": [
      "Lecture 2: Survey Sampling, Introduction"
    ]
  },
  {
    "objectID": "lectures/macros.html",
    "href": "lectures/macros.html",
    "title": "",
    "section": "",
    "text": "\\[\n\\newcommand{\\trans}{^\\mathsf{T}}\n\\newcommand{\\eps}{\\epsilon}\n\\]"
  },
  {
    "objectID": "lectures/lecture-3.html",
    "href": "lectures/lecture-3.html",
    "title": "Parameter Estimation: Method of Moments",
    "section": "",
    "text": "So far, we have estimated population parameters, mostly the mean and variance, but our analysis has been distribution-free. We haven’t made any assumptions other than our population being finite, and our sample being a simple random sample.\nNow, we are going to think about parametric models, that is we are going to assume that our populations are infinite, with parametric probability distributions, but unknown parameters.\nWe would use these when we have some reason to believe that our data is being generated with some particular distribution, and we try to figure out the parameters of the distribution, given a sample from this parametric distribution.\nFor instance, counts of goals scored in the soccer world cup, or counts of traffic accidents - or counts of cavalry officers in the Prussian army being kicked to death by their horses - these have been shown to follow a Poisson distribution. Later in the course, we will discuss nonparametric models as well, but it is useful to understand parametric models first, as these are widely used.\nWhen we talk about a family of probability distributions we refer to a class of probability distributions indexed by its parameter values. The shape of the distributions can vary (a lot!) depending on the values of the parameters, but the form of the distribution (given by the pdf or cdf) is determined by the class or the family. Many families of probability distributions, such as Poisson, or Gamma, or Gaussian, are determined by some small number of parameters. For the Poisson, you need the parameter \\(\\lambda\\), which would be the average count, for the Gamma distribution you need two parameters, \\(\\alpha\\), called the shape parameter, and \\(\\lambda\\), called the rate parameter. (Some texts will use \\(\\beta = 1/\\lambda\\) and call it the scale parameter.) The Gaussian family of distributions depends on two parameters, \\(\\mu\\) and \\(\\sigma\\). This means that when we are estimating the parameters of a probability distribution, given sample data, we might need to estimate a single parameter, or we might need to estimate a vector consisting of two or more parameters.\nFor example, here is a figure from Wikipedia’s page on the Normal distribution showing the probability density functions for various values of the parameters \\(\\mu\\) and \\(\\sigma\\).\n\n\n\nNormal family\n\n\nThe pdf has the same form for all of the density curves:\n\\[\nf(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n\\]\nEven when we are interested in only one parameter - the parameter of interest - we may need to estimate the remaining parameters in the model in order to estimate it. In some cases, the parameter of interest itself is a complicated function of the underlying model parameters, as in the following example from (Wasserman 2004):\nExample Let \\(X_1, X_2, \\ldots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2).\\) The parameter is a vector \\(\\mathbf{\\theta} = (\\mu, \\sigma)\\) where \\(\\sigma &gt; 0\\). Suppose that \\(X_i\\) is ther outcome of a blood test and we are interested in \\(\\tau\\), the fraction of the population whose test score is greater than 1. If \\(Z \\sim \\mathcal{N}(0,1)\\), then \\[\n\\begin{align*}\n\\tau &= P(X &gt; 1)\\\\\n&= 1-P(X&lt;1)\\\\\n&= 1 - P\\left(\\frac{X-\\mu}{\\sigma} &gt; \\frac{1-\\mu}{\\sigma}\\right)\\\\\n&= 1 - \\Phi\\left(\\frac{1-\\mu}{\\sigma}\\right)\\\\\n\\end{align*}\n\\] Therefore, the parameter of interest is \\(\\tau = T(\\mu, \\sigma) = \\displaystyle 1 - \\Phi\\left(\\frac{1-\\mu}{\\sigma}\\right)\\), where \\(T\\) denotes that \\(\\tau\\) is a function of \\(\\mu\\) and \\(\\sigma\\).\n\n\n\n\n\nAmericium 241 emitting alpha particles\n\n\n“Records of emissions of alpha particles from radioactive sources show that the number of emissions per unit time is not constant but fluctuates in a seemingly random fashion”.\nThis is an example of what we will do in this chapter. We will assume some distribution, get a data sample, and then use the data sample to estimate the parameter of the distribution we assumed. Since the underlying rate of emission was assumed to be constant, it was assumed that the emissions followed a Poisson distribution. In fact, for this very reason - that the number of emissions per unit time fluctuate randomly, but there is an underlying constant rate - the Poisson distribution is often used to model radioactive decay. Recall the conditions for using the Poisson distribution to model the number of events in time (or space) are that:\n\nThe underlying rate of events is constant in time (or space),\nThe number of events that occur in disjoint intervals are independent, and\nThere cannot be multiple events at the same instant.\n\nResearchers observed the radioactive decay of Americium 241 and the consequent emission of alpha particles, recording the time between successive emissions. They recorded 1,207 intervals of 10 seconds each, and counted the number of emissions in each of these intervals.\nThe table below, reproduced from (Rice 2006) shows emission counts \\(n\\) in the first column. In the second column is the number of intervals the researchers observed that had \\(n\\) emissions, for each \\(n\\) listed. For example, there were 28 10-second intervals observed that had 3 emissions each, 56 intervals that had 4 emissions each, etc. To compute the expected count, we need a value for the Poisson rate \\(\\lambda\\). The observed mean emission rate which was the total number of emissions divided by the total amount of time was recorded to be 0.8392 seconds. We use this observed mean emission rate to compute an estimate for \\(\\lambda\\) in the probability mass function of the Poisson distribution. Now, the 1,207 counts are the 1,207 realizations of a Poisson random variable with rate \\(\\lambda\\), where \\(\\lambda\\) is the expected number of emissions in 10 seconds,making our estimate, \\(\\hat{\\lambda} = 0.8392 \\times 10 = 8.392\\).\nFor example, how many intervals do we expect with exactly 3 emissions? Each interval has 4 emissions or not 4 emissions. So the number of intervals out of 1,207 total intervals with exactly 4 emissions is a Binomial random variable, with parameters \\(n = 1207\\), and \\(p = P(\\) exactly 4 emissions\\()\\) = \\(\\displaystyle \\dfrac{\\lambda^k e^{-\\lambda}}{k!} \\approx \\dfrac{\\hat{\\lambda}^k  e^{-\\hat{\\lambda}}}{k!}\\). If we plug in \\(k=4\\) and \\(\\hat{\\lambda} = 8.392\\), we get that \\(p \\approx 0.0468\\). Therefore, the expected number of intervals with exactly 4 emissions in \\(np = 1207\\times 0.0468 = 56.4876\\approx 56.5\\).\nFor the first row, since we are combining intervals with exactly 0 or 1 or 2 counts, the Binomial parameter \\(p\\) is given by \\(P(\\) exactly 0 OR exactly 1 OR exactly 2 emissions \\()\\), so we have to add the probabilities of each of these to get \\(p\\). We can see that the expected counts are not too far off from the observed counts. In chapter 9 we will discuss how to quantify the notion of “not too far off”.\n\nObserved and Expected Interval Counts for \\(n\\) Alpha Particle Emissions\n\n\n\\(n\\)\nObserved Number of Intervals\nExpected Number of Intervals\n\n\n\n\n0-2\n18\n12.2\n\n\n3\n28\n27.0\n\n\n4\n56\n56.5\n\n\n5\n105\n94.9\n\n\n6\n126\n132.7\n\n\n7\n146\n159.1\n\n\n8\n164\n166.9\n\n\n9\n161\n155.6\n\n\n10\n123\n130.6\n\n\n11\n101\n99.7\n\n\n12\n74\n69.7\n\n\n13\n53\n45.0\n\n\n14\n23\n27.0\n\n\n15\n15\n15.1\n\n\n16\n9\n7.9\n\n\n17+\n5\n7.1",
    "crumbs": [
      "Lecture 4: Method of Moments"
    ]
  },
  {
    "objectID": "lectures/lecture-3.html#introduction",
    "href": "lectures/lecture-3.html#introduction",
    "title": "Parameter Estimation: Method of Moments",
    "section": "",
    "text": "So far, we have estimated population parameters, mostly the mean and variance, but our analysis has been distribution-free. We haven’t made any assumptions other than our population being finite, and our sample being a simple random sample.\nNow, we are going to think about parametric models, that is we are going to assume that our populations are infinite, with parametric probability distributions, but unknown parameters.\nWe would use these when we have some reason to believe that our data is being generated with some particular distribution, and we try to figure out the parameters of the distribution, given a sample from this parametric distribution.\nFor instance, counts of goals scored in the soccer world cup, or counts of traffic accidents - or counts of cavalry officers in the Prussian army being kicked to death by their horses - these have been shown to follow a Poisson distribution. Later in the course, we will discuss nonparametric models as well, but it is useful to understand parametric models first, as these are widely used.\nWhen we talk about a family of probability distributions we refer to a class of probability distributions indexed by its parameter values. The shape of the distributions can vary (a lot!) depending on the values of the parameters, but the form of the distribution (given by the pdf or cdf) is determined by the class or the family. Many families of probability distributions, such as Poisson, or Gamma, or Gaussian, are determined by some small number of parameters. For the Poisson, you need the parameter \\(\\lambda\\), which would be the average count, for the Gamma distribution you need two parameters, \\(\\alpha\\), called the shape parameter, and \\(\\lambda\\), called the rate parameter. (Some texts will use \\(\\beta = 1/\\lambda\\) and call it the scale parameter.) The Gaussian family of distributions depends on two parameters, \\(\\mu\\) and \\(\\sigma\\). This means that when we are estimating the parameters of a probability distribution, given sample data, we might need to estimate a single parameter, or we might need to estimate a vector consisting of two or more parameters.\nFor example, here is a figure from Wikipedia’s page on the Normal distribution showing the probability density functions for various values of the parameters \\(\\mu\\) and \\(\\sigma\\).\n\n\n\nNormal family\n\n\nThe pdf has the same form for all of the density curves:\n\\[\nf(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n\\]\nEven when we are interested in only one parameter - the parameter of interest - we may need to estimate the remaining parameters in the model in order to estimate it. In some cases, the parameter of interest itself is a complicated function of the underlying model parameters, as in the following example from (Wasserman 2004):\nExample Let \\(X_1, X_2, \\ldots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2).\\) The parameter is a vector \\(\\mathbf{\\theta} = (\\mu, \\sigma)\\) where \\(\\sigma &gt; 0\\). Suppose that \\(X_i\\) is ther outcome of a blood test and we are interested in \\(\\tau\\), the fraction of the population whose test score is greater than 1. If \\(Z \\sim \\mathcal{N}(0,1)\\), then \\[\n\\begin{align*}\n\\tau &= P(X &gt; 1)\\\\\n&= 1-P(X&lt;1)\\\\\n&= 1 - P\\left(\\frac{X-\\mu}{\\sigma} &gt; \\frac{1-\\mu}{\\sigma}\\right)\\\\\n&= 1 - \\Phi\\left(\\frac{1-\\mu}{\\sigma}\\right)\\\\\n\\end{align*}\n\\] Therefore, the parameter of interest is \\(\\tau = T(\\mu, \\sigma) = \\displaystyle 1 - \\Phi\\left(\\frac{1-\\mu}{\\sigma}\\right)\\), where \\(T\\) denotes that \\(\\tau\\) is a function of \\(\\mu\\) and \\(\\sigma\\).\n\n\n\n\n\nAmericium 241 emitting alpha particles\n\n\n“Records of emissions of alpha particles from radioactive sources show that the number of emissions per unit time is not constant but fluctuates in a seemingly random fashion”.\nThis is an example of what we will do in this chapter. We will assume some distribution, get a data sample, and then use the data sample to estimate the parameter of the distribution we assumed. Since the underlying rate of emission was assumed to be constant, it was assumed that the emissions followed a Poisson distribution. In fact, for this very reason - that the number of emissions per unit time fluctuate randomly, but there is an underlying constant rate - the Poisson distribution is often used to model radioactive decay. Recall the conditions for using the Poisson distribution to model the number of events in time (or space) are that:\n\nThe underlying rate of events is constant in time (or space),\nThe number of events that occur in disjoint intervals are independent, and\nThere cannot be multiple events at the same instant.\n\nResearchers observed the radioactive decay of Americium 241 and the consequent emission of alpha particles, recording the time between successive emissions. They recorded 1,207 intervals of 10 seconds each, and counted the number of emissions in each of these intervals.\nThe table below, reproduced from (Rice 2006) shows emission counts \\(n\\) in the first column. In the second column is the number of intervals the researchers observed that had \\(n\\) emissions, for each \\(n\\) listed. For example, there were 28 10-second intervals observed that had 3 emissions each, 56 intervals that had 4 emissions each, etc. To compute the expected count, we need a value for the Poisson rate \\(\\lambda\\). The observed mean emission rate which was the total number of emissions divided by the total amount of time was recorded to be 0.8392 seconds. We use this observed mean emission rate to compute an estimate for \\(\\lambda\\) in the probability mass function of the Poisson distribution. Now, the 1,207 counts are the 1,207 realizations of a Poisson random variable with rate \\(\\lambda\\), where \\(\\lambda\\) is the expected number of emissions in 10 seconds,making our estimate, \\(\\hat{\\lambda} = 0.8392 \\times 10 = 8.392\\).\nFor example, how many intervals do we expect with exactly 3 emissions? Each interval has 4 emissions or not 4 emissions. So the number of intervals out of 1,207 total intervals with exactly 4 emissions is a Binomial random variable, with parameters \\(n = 1207\\), and \\(p = P(\\) exactly 4 emissions\\()\\) = \\(\\displaystyle \\dfrac{\\lambda^k e^{-\\lambda}}{k!} \\approx \\dfrac{\\hat{\\lambda}^k  e^{-\\hat{\\lambda}}}{k!}\\). If we plug in \\(k=4\\) and \\(\\hat{\\lambda} = 8.392\\), we get that \\(p \\approx 0.0468\\). Therefore, the expected number of intervals with exactly 4 emissions in \\(np = 1207\\times 0.0468 = 56.4876\\approx 56.5\\).\nFor the first row, since we are combining intervals with exactly 0 or 1 or 2 counts, the Binomial parameter \\(p\\) is given by \\(P(\\) exactly 0 OR exactly 1 OR exactly 2 emissions \\()\\), so we have to add the probabilities of each of these to get \\(p\\). We can see that the expected counts are not too far off from the observed counts. In chapter 9 we will discuss how to quantify the notion of “not too far off”.\n\nObserved and Expected Interval Counts for \\(n\\) Alpha Particle Emissions\n\n\n\\(n\\)\nObserved Number of Intervals\nExpected Number of Intervals\n\n\n\n\n0-2\n18\n12.2\n\n\n3\n28\n27.0\n\n\n4\n56\n56.5\n\n\n5\n105\n94.9\n\n\n6\n126\n132.7\n\n\n7\n146\n159.1\n\n\n8\n164\n166.9\n\n\n9\n161\n155.6\n\n\n10\n123\n130.6\n\n\n11\n101\n99.7\n\n\n12\n74\n69.7\n\n\n13\n53\n45.0\n\n\n14\n23\n27.0\n\n\n15\n15\n15.1\n\n\n16\n9\n7.9\n\n\n17+\n5\n7.1",
    "crumbs": [
      "Lecture 4: Method of Moments"
    ]
  },
  {
    "objectID": "lectures/lecture-3.html#parameter-estimation",
    "href": "lectures/lecture-3.html#parameter-estimation",
    "title": "Parameter Estimation: Method of Moments",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n\nSuppose our population is some parametric distribution with pmf or pdf \\(f(x \\vert \\theta)\\), where \\(\\theta\\) is the unknown parameter that we want to estimate.\nWe obtain an IID sample \\(X_1, X_2, \\ldots, X_n\\) from this distribution, so \\(X_i \\sim (x \\vert \\theta)\\).\nThe joint pmf or pdf of \\(X_1, X_2, \\ldots, X_n\\) is given by \\(f(x_1, x_2, \\ldots, x_n)\\). Because this probability depends on \\(\\theta\\), we write the pdf (or pmf, as the case may be), as \\(f(x_1, x_2, \\ldots, x_n \\vert \\theta)\\).\nFor simplicity, let’s assume that we have a discrete distribution, so that\n\n\\[\n\\begin{align*}\nf(x_1, x_2, \\ldots, x_n \\vert \\theta) &= P(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n)\\\\\n        &= P(X_1 = x_1)P(X_2 = x_2)\\ldots P(X_n = x_n)\\\\\n        &= f(x_1\\vert \\theta)f(x_2\\vert \\theta)\\ldots f(x_n\\vert \\theta)\\\\\n        &= \\prod_{i=1}^n f(x_i\\vert \\theta)\n\\end{align*}\n\\] - Any estimator of \\(\\theta\\) will be a function of the sample \\(X_1, X_2, \\ldots, X_n\\), which means that the estimator will itself be a random variable. Therefore it will have a probability distribution.\n\nJust like in chapter 7, we call the estimator \\(\\hat{\\theta}\\), and its distribution is called the sampling distribution of \\(\\hat{\\theta}\\).\nWe will need to approximate this distribution, since we need an idea of the error of our estimator, which is governed by its standard error \\(SE(\\hat{\\theta})\\), that is, we denote the square root of \\(\\mathrm{Var}(\\hat{\\theta})\\) by \\(SE(\\hat{\\theta})\\).\nWe will study the following two methods:\n\n\nMethod of moments\nMaximum Likelihood Estimation\n\n\nWe would like to minimize the standard error of our estimator, so we will choose the estimator whose distribution is most concentrated about the true value \\(\\theta\\). That is, we will choose (from among the unbiased estimators) the estimator with the smallest standard error.",
    "crumbs": [
      "Lecture 4: Method of Moments"
    ]
  },
  {
    "objectID": "lectures/lecture-3.html#method-of-moments",
    "href": "lectures/lecture-3.html#method-of-moments",
    "title": "Parameter Estimation: Method of Moments",
    "section": "Method of Moments",
    "text": "Method of Moments\nLet \\(\\mu_k\\) denote the \\(k\\)th moment of the random variable \\(X\\). That is, \\(\\mu_k = E(X^k)\\). The first moment is the mean:\n\\[\n\\mu_1 = E(X).\n\\]\nThe second moment isn’t the variance, but we can get the variance from the first and second moments: \\[\n\\mu_2 = E(X^2) \\Rightarrow \\mathrm{Var}(X) = \\mu_2 - \\mu_1^2\n\\]\nNow, suppose we have an IID random sample \\(X_1, X_2, \\ldots, X_n\\). Define the \\(k\\)th sample moment \\(\\hat{\\mu}_k\\) by \\[\n\\hat{\\mu}_k = \\frac{1}{n} \\sum_{i =1 }^n X_i^k\n\\] For example, \\(\\hat{\\mu}_1\\) is the sample mean \\(\\overline{X}\\). We can use \\(\\hat{\\mu}_k\\) as an estimate for \\(\\mu_k = E(X^k) = E(X^k \\vert \\theta)\\). The method of moments estimates parameters by finding expressions for them in terms of the moments of the lowest possible order, and then plugging in the sample moments into the expression. We usually need as many moments as there are unknown parameter values.\nThere are 3 steps to this method:\n\nFind the moments \\(E(X^k)\\).\nSolve for \\(\\theta\\) in terms of the moments, that is get an expression that looks like \\(\\theta = h(\\mu_1, \\mu_2, \\ldots, \\mu_k)\\).\nPlug in the sample moments to get \\(\\hat{\\theta}\\), so we get \\(\\hat{\\theta} = h(\\hat{\\mu}_1, \\hat{\\mu}_2, \\ldots, \\hat{\\mu}_k)\\)\n\n\nExamples\nExample 1\n\\(X_1, X_2, \\ldots, X_n \\sim Bernoulli(p)\\)\nSo here, \\(\\theta = p\\)\n\n\nCheck your answer!\n\nStep 1: Find the moments. Note that we have just one parameter, so the first moment should suffice.\n\\(E(X_i) = p = \\theta\\).\nStep 2: Therefore \\(\\theta = \\mu_1\\) and we don’t need any more moments. Now $_1 = = .\nStep 3: $ = _1 = .\n/details&gt;\nExample 2\n\\(X_1, X_2, \\ldots, X_n \\sim Exponential(\\lambda)\\).\nStep 1. \\(\\mu_1 = E(X_i) = \\frac{1}{\\lambda}\\)\nStep 2: Solve for \\(\\lambda\\)\n\\(\\lambda = \\dfrac{1}{\\mu_1} \\Rightarrow \\hat{\\lambda} = \\dfrac{1}{\\hat{\\mu}_1} = \\dfrac{1}{\\overline{X}}\\)\n\n[Rice (2006); Wasserman (2004); Pimentel (2024);]",
    "crumbs": [
      "Lecture 4: Method of Moments"
    ]
  },
  {
    "objectID": "lectures/lecture-3.html#references",
    "href": "lectures/lecture-3.html#references",
    "title": "Parameter Estimation: Method of Moments",
    "section": "References",
    "text": "References\n\n\nPimentel, Sam. 2024. “STAT 135 Lecture Slides.” Lecture slides (shared privately).\n\n\nRice, John A. 2006. Mathematical Statistics and Data Analysis. 3rd ed. Duxbury Press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. New York: Springer.",
    "crumbs": [
      "Lecture 4: Method of Moments"
    ]
  },
  {
    "objectID": "lectures/lecture-4.html",
    "href": "lectures/lecture-4.html",
    "title": "Parameter Estimation",
    "section": "",
    "text": "[UNDER CONSTRUCTION]",
    "crumbs": [
      "Lecture 5: Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 135: Concepts of Statistics",
    "section": "",
    "text": "Ed\n\n  DataHub\n\n  Gradescope\n\n  bCourses\n\n\nNo matching items",
    "crumbs": [
      "Home / Schedule"
    ]
  },
  {
    "objectID": "index.html#welcome-to-stat-135",
    "href": "index.html#welcome-to-stat-135",
    "title": "Stat 135: Concepts of Statistics",
    "section": "Welcome to Stat 135!",
    "text": "Welcome to Stat 135!\n\n\n\n\n\n\nIMPORTANT\n\n\n\nIf you need to be manually added to bcourses, please reach out to Thomas.",
    "crumbs": [
      "Home / Schedule"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Stat 135: Concepts of Statistics",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n   Week 1\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           \n           \n           \n           Jan 21:\n           \n           \n              Lecture 1 Overview\n           \n           \n                \n                \n                  \n                    Syllabus\n                  \n                  \n                  \n                    Diagnostic Test\n                  \n                  \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           \n           \n           \n           Jan 23:\n           \n           \n              Lecture 2 Survey Sampling Part 1\n           \n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   Week 2\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           \n           \n           \n           Jan 26:\n           \n           \n              Lecture 3 Survey Sampling Part 2\n           \n           \n                \n                \n                  \n                    Homework 1\n                  \n                  \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           \n           \n           \n           Jan 27:\n           \n           \n              Lecture 4 Parameter Estimation (Method of Moments)\n           \n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           \n           \n           \n           Jan 30:\n           \n           \n              Lecture 5 Parameter Estimation (MLE)\n           \n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Home / Schedule"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Description",
    "section": "",
    "text": "This is one of the three foundational courses of the Statistics Major, the other two being Stat 134 (Concepts of Probability) and Stat 133 (Concepts of Computing). While Stat 134, which is a serious prerequisite for Stat 135, is a course in probability, Stat 135 is a course in statistical inference. It is a comprehensive survey course in statistical theory and methodology. Topics include parameter estimation, hypothesis testing, statistical tests (parametric and non parametric) and linear regression (single and an example of multiple). We will cover most of the content of chapters 7 through 14 in the text Mathematical Statistics and Data Analysis by John Rice (3rd Edition), with a brief look at the content in chapters 5 and 6.\n\n\nBy the end of the semester, you should be able to:\n\nClearly interpret point estimates, confidence intervals, and hypothesis tests for an audience without statistical training.\nConstruct common estimators, statistical tests and confidence interval procedures using probability theory.\nEvaluate the relative strengths and limitations of several estimation or inference procedures for the same problem using mathematical concepts including unbiasedness, efficiency, and power.\nRecommend an approach and carry out estimation and inference for canonical statistics problems including tests of association between two variables and fitting probability distributions to univariate data.\n\n\n\n\n\nSTAT 134 or an equivalent course in probability theory. Do NOT take 134 and 135 concurrently!!\nMultivariable calculus, especially Lagrange multipliers.\nFamiliarity with moment-generating functions.\nFamiliarity with basic R concepts equivalent to the first ~6 weeks of Stat 133. Note that assignments involving computing must be completed in R.\nFamiliarity with linear algebra (matrix operations, inverses, and eigenvalues) for chapter 14.\n\n\n\n\n\nMathematical Statistics and Data Analysis (3rd Edition), by John Rice: This is the main text that we will follow, and exercises will mostly be from here. Make sure that you have the third edition.\nR for Data Science, by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\nStatistics, by Freedman, Pisani, and Purves: This is the text for Stat 2. It has no coding, but nevertheless is a wonderful book to read, and I strongly recommend reading through it to improve your understanding of statistics.\nVeridical Data Science, by Bin Yu and Rebecca Barter: This is a new book written by Prof. Yu and R. Barter and introduces a framework to practice data science. Though it doesn’t really talk about statistical inference, which is the focus of our class, it discusses how to apply the methods to get reproducible and trustworthy results.\nStat Labs, by Deborah Nolan and Terry Speed: Statistical topics are introduced via case studies.\n\nAs we progress through the course, I may add more books to this list.",
    "crumbs": [
      "About Stat 135"
    ]
  },
  {
    "objectID": "about.html#about-stat-135-concepts-of-statistics",
    "href": "about.html#about-stat-135-concepts-of-statistics",
    "title": "Description",
    "section": "",
    "text": "This is one of the three foundational courses of the Statistics Major, the other two being Stat 134 (Concepts of Probability) and Stat 133 (Concepts of Computing). While Stat 134, which is a serious prerequisite for Stat 135, is a course in probability, Stat 135 is a course in statistical inference. It is a comprehensive survey course in statistical theory and methodology. Topics include parameter estimation, hypothesis testing, statistical tests (parametric and non parametric) and linear regression (single and an example of multiple). We will cover most of the content of chapters 7 through 14 in the text Mathematical Statistics and Data Analysis by John Rice (3rd Edition), with a brief look at the content in chapters 5 and 6.\n\n\nBy the end of the semester, you should be able to:\n\nClearly interpret point estimates, confidence intervals, and hypothesis tests for an audience without statistical training.\nConstruct common estimators, statistical tests and confidence interval procedures using probability theory.\nEvaluate the relative strengths and limitations of several estimation or inference procedures for the same problem using mathematical concepts including unbiasedness, efficiency, and power.\nRecommend an approach and carry out estimation and inference for canonical statistics problems including tests of association between two variables and fitting probability distributions to univariate data.\n\n\n\n\n\nSTAT 134 or an equivalent course in probability theory. Do NOT take 134 and 135 concurrently!!\nMultivariable calculus, especially Lagrange multipliers.\nFamiliarity with moment-generating functions.\nFamiliarity with basic R concepts equivalent to the first ~6 weeks of Stat 133. Note that assignments involving computing must be completed in R.\nFamiliarity with linear algebra (matrix operations, inverses, and eigenvalues) for chapter 14.\n\n\n\n\n\nMathematical Statistics and Data Analysis (3rd Edition), by John Rice: This is the main text that we will follow, and exercises will mostly be from here. Make sure that you have the third edition.\nR for Data Science, by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\nStatistics, by Freedman, Pisani, and Purves: This is the text for Stat 2. It has no coding, but nevertheless is a wonderful book to read, and I strongly recommend reading through it to improve your understanding of statistics.\nVeridical Data Science, by Bin Yu and Rebecca Barter: This is a new book written by Prof. Yu and R. Barter and introduces a framework to practice data science. Though it doesn’t really talk about statistical inference, which is the focus of our class, it discusses how to apply the methods to get reproducible and trustworthy results.\nStat Labs, by Deborah Nolan and Terry Speed: Statistical topics are introduced via case studies.\n\nAs we progress through the course, I may add more books to this list.",
    "crumbs": [
      "About Stat 135"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Lectures will meet on MWF from 2 - 3 pm in Evans 10. Your instructor is Shobhana Murali Stoyanov.\nLectures will cover core theory and concepts, with supporting data analysis examples. To get the full benefit of lecture, it is best to read the supporting material ahead of time. When slides or R code are shown in class, they will be posted online after class. However, lectures will not always have associated slides. I will not be posting recordings of the lectures, but if you have to miss class due to an emergency or unavoidable circumstance, the class notes and text should suffice. Please do come and see me so we can review what you have missed.\nLecture attendance will be measured using occasional quizzes. There will be roughly 8-10 of these handed out in lecture throughout the semester. You need to complete five of them to get the full attendance credit.\n\n\n\nThe sections meet once a week, on Tuesdays, for two hours. The individual section times will be listed in the course calendar. Section time will be spent working on practice problems. Since some problems will involve computing you should plan to always bring your laptop. You may attend a lab for which you are not enrolled (physical space permitting). Labs will not be recorded so attendance is strongly recommended.\nYour GSIs are Chuao Dong, Thomas Lee, and Toby Roemer.\n\n\n\nProf. Stoyanov and one of the GSIs will hold individual office hours each week. In addition, there will be group office hours in one of the 3rd floor classrooms in Evans, the room and hours TBA. You can go to these to get questions answered about homework or any concepts discussed in class, or help with your coding assignments.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#logistics",
    "href": "syllabus.html#logistics",
    "title": "Syllabus",
    "section": "",
    "text": "Lectures will meet on MWF from 2 - 3 pm in Evans 10. Your instructor is Shobhana Murali Stoyanov.\nLectures will cover core theory and concepts, with supporting data analysis examples. To get the full benefit of lecture, it is best to read the supporting material ahead of time. When slides or R code are shown in class, they will be posted online after class. However, lectures will not always have associated slides. I will not be posting recordings of the lectures, but if you have to miss class due to an emergency or unavoidable circumstance, the class notes and text should suffice. Please do come and see me so we can review what you have missed.\nLecture attendance will be measured using occasional quizzes. There will be roughly 8-10 of these handed out in lecture throughout the semester. You need to complete five of them to get the full attendance credit.\n\n\n\nThe sections meet once a week, on Tuesdays, for two hours. The individual section times will be listed in the course calendar. Section time will be spent working on practice problems. Since some problems will involve computing you should plan to always bring your laptop. You may attend a lab for which you are not enrolled (physical space permitting). Labs will not be recorded so attendance is strongly recommended.\nYour GSIs are Chuao Dong, Thomas Lee, and Toby Roemer.\n\n\n\nProf. Stoyanov and one of the GSIs will hold individual office hours each week. In addition, there will be group office hours in one of the 3rd floor classrooms in Evans, the room and hours TBA. You can go to these to get questions answered about homework or any concepts discussed in class, or help with your coding assignments.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#stat-scholars-program",
    "href": "syllabus.html#stat-scholars-program",
    "title": "Syllabus",
    "section": "Stat Scholars Program",
    "text": "Stat Scholars Program\nThe new Stat Scholars Program will be available to Stat 135 students to provide community, academic support, and more such as answers to your questions about the stat major, new directions in statistics, research being done in the stat department etc. Applications will be sent out soon, begin checking your mail next week.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\n\nHomework Assignments\nHomework assignments will be due roughly every week or so during the semester, starting in the second week. Homework will be posted to Gradescope usually by Friday, and will generally be due the following Friday at 11:59 pm. Homework will be a combination of analytical and computational exercises done “by hand” and data analysis using R. If you turn in your homework late, you will get half credit for homework that is up to 24 hours late (if you have not requested an extension), and no credit after that.\n\n\nQuizzes\nThere will be four 50-minute quizzes to test your understanding of homework and lecture. You will take the quizzes in the CBTF (Computer-Based Testing Facility) in 200 Sutardja Dai Hall. The dates of the quizzes are: Feb 12/13, Feb 26/27, Apr 9/10, and Apr 30/May 1. You will have to schedule your quiz during the given dates at the CBTF. We do not schedule your quiz for you. Note that DSP students will also take it in the CBTF and will get their extended time. We will drop your lowest quiz score, and you may retake ** at most one** quiz, also in the CBTF, during the week following the original quiz dates. Dates for each retake will be announced later, and you will have to fill out a form to be registered for the retake.\n\n\nExams\nThere will be one midterm exam and the final exam.\n\nThe midterm exam will be a two hour exam which will be held in the evening from 7-9 pm  on Wednesday, March 11 in VLSB 2050.\nThe final exam time and day are Tuesday, May 12, 11:30 am - 2:30 pm (group 6). If you do not take the final exam, you will not pass the class.\n\n\n\nExtra Credit Opportunities\nExtra credit is assigned at the end of the semester after the grade bins have been determined. Ways to get extra credit (it will be added to your overall course percentage):\n\nDiagnostic test: 0.25% for submission with some clear attempt at solving the questions.\nEdStem: The top 5 student answerers will get up to 0.5% extra credit.\n\n\n\nAttendance Quizzes\nAs stated above, there will be 8-10 attendance quizzes handed out in lecture throughout the semester. These will be graded on completion, and you will need to complete five of them to get all the attendance points.\n\n\nOverall Score\nYour letter grade for the course will be based a weighted average of your assessments, as follows:\n\nHomework (each assignment weighted equally, drop two lowest): 15%\nQuizzes: 15% (equally weighted, drop lowest)\nMidterm: 25% (can be clobbered by the final)\nFinal exam: 40% or 65% (if higher than the midterm)\nAttendance quizzes: 5%\n\nRough grade bins: Students earning a course average of at least 88% are guaranteed to receive an A- or better, students earning a course average of at least 75% are guaranteed a B- or better, and students earning at least 60% on their course average are guaranteed at least a C-. These bins might move down, depending on how the class does.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-sites-to-bookmark",
    "href": "syllabus.html#important-sites-to-bookmark",
    "title": "Syllabus",
    "section": "Important sites to bookmark",
    "text": "Important sites to bookmark\n\nR Datahub\nAll coding in this class will be done in R. You may download the desktop version of RStudio if you wish, but it might be easier to use the datahub. It is up to you. Just make sure you have the latest version of R and RStudio, if you would like to download it to your laptop.\n\n\nBcourses\nImportant coursewide announcements will be sent out on bcourses.\n\n\nEd Discussion\nI have created a Ed discussion page for this course, which you can access through the link in bCourses (so you will be logged in). This is an online forum to ask questions to fellow students and answer other students’ questions. The GSIs and I will have access to the forum and may endorse or occasionally answer questions but this is primarily a forum for students to help each other – if you need an instructor or GSI’s assistance please attend office hours. Extra credit of up to 0.5% on the final course score will be awarded to the five students who have responded to the most questions on Ed by the end of the semester (with reasonable attempts at answering the questions).\n\n\nCBTF\nThis semester, you will be taking your quizzes in the Computer-Based Testing Facility in Sutardja Dai Hall. In order to take the test, you will have to log into the course site using your Calnet ID. I will post a practice quiz so that you can get the hang of how to do the scheduling etc. Meanwhile, take a look at the page “CBTF Getting Started for Students”.\n\n\nGradescope\nHomework assignments, take-home exams, and regrade requests (see Policies section below) will be submitted through Gradescope, which you can also access through the link in bCourses (you should not add yourself to Gradescope, as the roster has already been uploaded). You can view your grades here, as I will not be creating assignments on bCourses.\n\n\nFlextensions\nYou can request extensions of up to two days for up to two homework assignments, using this page, once assignments have been posted on Gradescope. You must make the request before the due date. If you have DSP accommodations, please send us a private message on Ed.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#academic-honesty-policy",
    "href": "syllabus.html#academic-honesty-policy",
    "title": "Syllabus",
    "section": "Academic Honesty Policy",
    "text": "Academic Honesty Policy\nThe student community at UC Berkeley has adopted the following Honor Code: “As a member of the UC Berkeley community, I act with honesty, integrity, and respect for others.”.\nMy expectation is that you will adhere to this code. Beyond the importance of respecting your fellow students, acting with integrity in completing course assignments helps ensure that they achieve their purpose, which is to help you learn and develop valuable statistical understanding and skills. Homework must be done independently. If you get stuck or want to explore alternative approaches, feel free to discuss issues with students or course staff at the homework parties or on Ed. You may ask an AI tool for help understanding a concept, or figuring out a coding error (but note that the answers are not always correct), but putting assignment questions into an AI tool and then copying and pasting/paraphrasing the answers will be considered plagiarism. It is also completely negates the purpose of the homework, which is to cement and internalize concepts from the course. It will not serve you for the exam either, when you have to tackle the problems on your own. I leave it to you to decide how much you would like to use genAI tools, but please use them with caution.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#emailed-policy",
    "href": "syllabus.html#emailed-policy",
    "title": "Syllabus",
    "section": "Email/Ed policy",
    "text": "Email/Ed policy\nIf you have a question for us, the fastest way to get a response is to post the question on Ed. If it deals with private matters, please make it a private post. If you are not comfortable writing a private post, come and see me. You can drop into office hours or make an appointment, either is fine.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#inclusivity-and-accommodation",
    "href": "syllabus.html#inclusivity-and-accommodation",
    "title": "Syllabus",
    "section": "Inclusivity and Accommodation",
    "text": "Inclusivity and Accommodation\nMy hope is to establish a learning environment in this course that welcomes diversity of thought, perspective, and experience, and to be respectful of your individual identity as a student. I am happy to use your preferred name and/or personal pronoun. If you feel uncomfortable as a result of anything that is said in class, or if you feel that your performance in the course is being impacted by experiences outside of class, please do not hesitate to reach out to me about your concerns. If you are uncomfortable being called on during class, just let me know.\nIn addition, if you need accommodations for any physical, psychological, or learning disability, please speak to me after class or during office hours. Please note that you must make arrangements in a timely manner through DSP so that I can make the appropriate accommodations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#possibility-of-revisions-to-course-policies",
    "href": "syllabus.html#possibility-of-revisions-to-course-policies",
    "title": "Syllabus",
    "section": "Possibility of revisions to course policies",
    "text": "Possibility of revisions to course policies\nAll course policies, including assessment, are subject to change during the course of the semester in response to unforeseen events including, but not limited to, campus shutdowns due to various reasons, power outages, forest fires, and medical emergencies among members of the course staff.",
    "crumbs": [
      "Syllabus"
    ]
  }
]