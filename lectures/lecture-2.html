<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Survey Sampling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-9ca244728582364b54b6a88cc4015eed.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="”https://siteimproveanalytics.com/js/siteanalyze_6294756.js”"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/lecture-2.html">Lecture 2: Survey Sampling, Introduction</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../assets/stat_bear.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Stat 135</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/berkeley-stat135/spring-2026" title="" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home / Schedule</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About Stat 135</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../syllabus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Syllabus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../staff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Staff</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../calendar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Calendar</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Lectures</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lecture-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 1: Syllabus &amp; Diagnostic Test</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lecture-2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Lecture 2: Survey Sampling, Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lecture-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 3: Survey Sampling, Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lecture-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 4: Method of Moments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lecture-4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 5: Maximum Likelihood Estimation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datasets</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../license.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">License</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#warm-up-problem" id="toc-warm-up-problem" class="nav-link" data-scroll-target="#warm-up-problem">Warm up problem</a></li>
  </ul></li>
  <li><a href="#definitions-and-vocabulary" id="toc-definitions-and-vocabulary" class="nav-link" data-scroll-target="#definitions-and-vocabulary">Definitions and vocabulary</a></li>
  <li><a href="#inference-in-sampling" id="toc-inference-in-sampling" class="nav-link" data-scroll-target="#inference-in-sampling">Inference in Sampling</a>
  <ul class="collapse">
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error">Mean Squared Error</a></li>
  <li><a href="#mathrmvaroverlinex-and-the-finite-population-correction" id="toc-mathrmvaroverlinex-and-the-finite-population-correction" class="nav-link" data-scroll-target="#mathrmvaroverlinex-and-the-finite-population-correction"><span class="math inline">\(\mathrm{Var}(\overline{X})\)</span> and the finite population correction</a></li>
  <li><a href="#estimating-the-population-variance" id="toc-estimating-the-population-variance" class="nav-link" data-scroll-target="#estimating-the-population-variance">Estimating the Population Variance</a></li>
  <li><a href="#the-asymptotic-sampling-distribution-of-the-sample-mean" id="toc-the-asymptotic-sampling-distribution-of-the-sample-mean" class="nav-link" data-scroll-target="#the-asymptotic-sampling-distribution-of-the-sample-mean">The (Asymptotic) Sampling Distribution of the Sample Mean</a></li>
  <li><a href="#confidence-intervals-for-the-population-mean" id="toc-confidence-intervals-for-the-population-mean" class="nav-link" data-scroll-target="#confidence-intervals-for-the-population-mean">Confidence intervals for the Population Mean</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Survey Sampling</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In this chapter, we look at the topic of <em>survey sampling</em>, which involves a particular type of <em>inference</em>, saying something about a population, given an observed subset of the population. By now, we are all very used to sample surveys, such as presidential approval polls, and polls on various issues such as: <em>What percentage of Republicans support vaccine requirements for children to attend public schools</em>. Pew Research investigated this question a couple of years ago, which they attempted to answer by taking a <strong>sample</strong> of Republican voters, and then drawing a conclusion about the <strong>population</strong> of Republican voters.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lec-2-images/pew-vaccine.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p>News outlets are constantly publishing polls, which are certainly not all the same quality. The famous FiveThirtyEight site, started by Nate Silver, is defunct now, but it was famous for its <em>pollster ratings</em>. You can read about their <a href="https://abcnews.go.com/538/best-pollsters-america/story?id=105563951">methodology</a> and here are their rankings from a couple of years ago.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lec-2-images/pollster-ratings-538-2024.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>FiveThirtyEight pollster ratings from January 2024</figcaption>
</figure>
</div>
<p>In our course, we learn a little bit about survey sampling, and if you like it, you might think about taking Stat 152 the next time it is offered in our department. Chapter 7 in our text discusses the probabilistic sampling techniques, in that each population <em>unit</em> has a <em>specified</em> probability of being included in the sample, which consists of randomly selected units from the population. Note that we make no distributional assumptions in this chapter. We will restrict our study to <strong>Simple Random Samples</strong>: every population unit has the same probability of being selected, and each particular sample of size <span class="math inline">\(n\)</span> has the same probability. That is, if the size of our population is <span class="math inline">\(N\)</span>, then each of the <span class="math inline">\(\displaystyle \binom{N}{n}\)</span> possible samples of size <span class="math inline">\(n\)</span> taken without replacement has the <em>same</em> probability.</p>
<section id="warm-up-problem" class="level3">
<h3 class="anchored" data-anchor-id="warm-up-problem">Warm up problem</h3>
<p>Consider the following problem:</p>
<p>You have a box containing 5 cards. Four of the cards are labeled with the number <span class="math inline">\(0\)</span> and one of them is labeled with the number <span class="math inline">\(1\)</span>.You pick <em>two</em> cards at random <strong>with</strong> replacment. Let <span class="math inline">\(Y\)</span> represent the <em>average</em> of the two cards.</p>
<ol type="1">
<li>What is the distribution of the random variable <span class="math inline">\(Y\)</span>? (<em>Hint</em>: Define <span class="math inline">\(X\)</span> to be the <em>sum</em> of the two cards. What is the distribution of <span class="math inline">\(X\)</span>?)</li>
</ol>
<details>
<summary>
Check your answer
</summary>
<p><span class="math inline">\(X \sim Bin(2, \dfrac{1}{5})\)</span>, and <span class="math inline">\(Y = X/2\)</span>.</p>
<p><span class="math inline">\(P(Y = 0) = P(X = 0) = \dfrac{16}{25}\)</span></p>
<p><span class="math inline">\(P(Y = \dfrac{1}{2}) = P(X = 1) = \dfrac{8}{25}\)</span></p>
<p><span class="math inline">\(P(Y = 1) = P(X = 2) = \dfrac{1}{25}\)</span></p>
</details>
<ol start="2" type="1">
<li>Compute <span class="math inline">\(E(Y)\)</span> and <span class="math inline">\(\mathrm{Var}(Y)\)</span>.</li>
</ol>
<details>
<summary>
Check your answer
</summary>
<p><span class="math inline">\(E(Y) = \dfrac{1}{5}\)</span> and <span class="math inline">\(\mathrm{Var}(Y) = \dfrac{2}{25}\)</span>.</p>
</details>
<p>Now what if I sample <em>without</em> replacement? Let <span class="math inline">\(Z\)</span> be the average of the two tickets in this case. What is the distribution of <span class="math inline">\(Z\)</span>?</p>
<details>
<summary>
Check your answer
</summary>
<p>Now the sum is <em>Hypergeometric</em>. (What are the parameters of the distribution?)</p>
<p>You can work out that <span class="math inline">\(P(Z = 0) = \dfrac{3}{5}, P(Z = \dfrac{1}{2}) = \dfrac{2}{5}\)</span>.</p>
<p>Why can’t <span class="math inline">\(Z\)</span> be <span class="math inline">\(1\)</span>?</p>
</details>
</section>
</section>
<section id="definitions-and-vocabulary" class="level2">
<h2 class="anchored" data-anchor-id="definitions-and-vocabulary">Definitions and vocabulary</h2>
<p>The figure below, adapted from Lohr’s book on sampling, shows that we have to be careful regarding the <em>scope</em> of our conclusions. We can only generalize from results from our sample to the <strong>sampled population</strong>, even if the <strong>target population</strong> (the population we are interested in) is something different!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lec-2-images/sampling-schematic.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%" alt="A schematic drawing target populations, sampling frame, sampled populations, etc."></p>
</figure>
</div>
<div class="def">
<p><strong>Population</strong>: the complete set of individuals or entities that we are interested in. We usually only have data on a subset of them (a <strong>sample</strong>). We will assume that our population is of (finite) size <span class="math inline">\(N\)</span>, and that associated with each member or <em>unit</em> of the population is some numerical value. We will denote these numbers by <span class="math inline">\(x_1, x_2, \ldots, x_N\)</span>. If the values of the <span class="math inline">\(x_i\)</span> are <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> then we are usually investigating the presence or absence of some characteristic, such as a particular party affiliation. In this case, our population is <em>dichotomous</em> or binary.</p>
</div>
<div class="def">
<p><strong>Parameter</strong>: any <em>quantifiable</em> feature of a population. For now, we will assume that the parameter is <strong>fixed but unknown</strong>.</p>
<p>For example: The mean age of all undergraduate students at UC Berkeley.</p>
</div>
<p>The most common population parameters that we are interested in are:</p>
<div class="def">
<p><strong>Population mean or average</strong>: this is denoted by <span class="math inline">\(\mu\)</span> and defined to be: <span class="math display">\[
\mu = \dfrac{1}{N}\sum_{i = 1}^N x_i
\]</span></p>
</div>
<div class="def">
<p><strong>Population proportion</strong>: This is just the population mean in the binary case, and we represent this special mean by <span class="math inline">\(p\)</span> rather than <span class="math inline">\(\mu\)</span>.</p>
</div>
<p>Other parameters that we will consider:</p>
<div class="def">
<p><strong>Population total</strong>: this is denoted by <span class="math inline">\(\tau\)</span> and defined to be: <span class="math display">\[
\tau = \sum_{i = 1}^N x_i = N\mu
\]</span> Note that for a binary population, <span class="math inline">\(\tau\)</span> represents how many population units possess the characteristic of interest.</p>
</div>
<div class="def">
<p><strong>Population variance</strong>: this is denoted by <span class="math inline">\(\sigma^2\)</span> and defined to be: <span class="math display">\[
\sigma^2 = \dfrac{1}{N}\sum_{i = 1}^N (x_i-\mu)^2
\]</span> The <strong>population standard deviation</strong> is the square root of the population variance.</p>
</div>
<p><strong>Exercise</strong>: Show that <span class="math inline">\(\sigma^2\)</span> reduces to <span class="math inline">\(\displaystyle \dfrac{1}{N}\sum_{i = 1}^N x_i^2 -\mu^2\)</span>, and if the <span class="math inline">\(x_i\)</span> are <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> only, then <span class="math inline">\(\sigma^2 = p(1-p)\)</span>.</p>
<strong>Exercise</strong> Going back to the results of the Pew Research survey shown at the beginning of these notes. What is the population and the parameter of interest?
<details>
<summary>
Check your answer
</summary>
<p>Population: US adults</p>
<p>Parameter: Percentage of US adults that think healthy children should be required to be vaccinated in order to attend public schools.</p>
</details>
<p><strong>Exercise</strong> Consider a population of size 4: <span class="math inline">\(\{x_1, x_2, x_3, x_4\}\)</span>.</p>
<ol type="a">
<li><p>If we use simple random sampling, how many samples of size 2 will we have? What would be the expected value of the sample mean? Is it equal to the population mean?</p></li>
<li><p>If, rather than a simple random sample, when <em>all</em> samples of size 2 are equally likely, we use a different probabilistic scheme for getting our samples of size 2: the following <strong>four</strong> samples are equally likely, and <em>only</em> these samples are possible: <span class="math inline">\(\{x_1, x_2\}, \{x_2, x_3\}, \{x_3, x_4\}, \{x_1, x_4\}\)</span>. What would be the expected value of the sample mean? Is it equal to the population mean?</p></li>
</ol>
</section>
<section id="inference-in-sampling" class="level2">
<h2 class="anchored" data-anchor-id="inference-in-sampling">Inference in Sampling</h2>
<p><strong>Inference</strong> involves using a <strong>sample</strong> to compute an <strong>estimate</strong> of a <strong>population parameter</strong>, and the population should always be defined in the context in which the results will be applied.</p>
<div class="def">
<p><strong>Estimator</strong>: The <strong>function</strong> (or algorithm) that maps sample data to a number.</p>
</div>
<div class="def">
<p><strong>Estimate</strong>: The <em>actual</em> observed value after applying the estimator on the sample (observed) data.</p>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lec-2-images/inference-sampling.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>Then of course the question we would have is how <em>good</em> is our estimator and therefore our estimate? We want <span class="math inline">\(\mu\)</span>, and we have the estimate <span class="math inline">\(\hat{\mu}\)</span>. How close is this estimate to the true value <span class="math inline">\(\mu\)</span>? We need a measure of <em>goodness</em> of our estimator. Note that our estimator is <strong>random</strong>. Each time we take a random sample, we will get a different value of the estimate. We want to know on <strong>average</strong>, what is the error of our estimator? To compute this, we need to consider the <strong>sampling distribution</strong> of our estimator. This is just a special name for the probability distribution of the estimator, which is a random variable. The randomness of the estimator is rooted in the randomness of the sampling. The spread of the probability distribution, measured by its <em>standard deviation</em> is one of the determinants of the accuracy of our estimator.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lec-2-images/sampling-dsn-estimator.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>If we would hit our target (the population parameter) on average (that means that the expected value of our estimator is the population parameter), then we only need to consider how much our estimator’s sampling distribution spreads about the mean. The tighter the spread (the smaller the standard deviation), the more accurate the estimator. Now, because we are measuring the <strong>error</strong> of our estimator, we call the square root of its variance the <strong>standard error</strong> rather than the standard deviation.</p>
<p>What if, though, the expected value of our estimator is <em>not</em> the target parameter? In this case the difference between the expected value of the estimator and the <em>true</em> value of the population parameter will also contribute to the error. Because of this, we use a measure of <em>goodness</em> of our estimate that incorporates both the spread (standard error) <em>and</em> the average distance from the parameter (we call this <strong>bias</strong>). This measure is called the <strong>Mean Squared Error</strong>.</p>
<section id="mean-squared-error" class="level3">
<h3 class="anchored" data-anchor-id="mean-squared-error">Mean Squared Error</h3>
<div class="def">
<p><strong>Mean Squared Error</strong>: The mean squared error is the expected value of the squared difference between the estimator <span class="math inline">\(\hat{\theta}\)</span> and the true value of the population parameter <span class="math inline">\(\theta\)</span>. We denoted it by <span class="math inline">\(MSE\)</span>: <span class="math display">\[
MSE = E\left[\left(\hat{\theta} - \theta \right)^2\right]
\]</span></p>
</div>
<div class="def">
<p><strong>Bias</strong>: The bias of an estimator is its distance, on average, from the true value of the population parameter: <span class="math display">\[
\operatorname{Bias}(\hat{\theta}) = E(\hat{\theta}) - \theta
\]</span> We call an estimator <strong>unbiased</strong> if the bias is 0, that is if <span class="math inline">\(E(\hat{\theta}) = \theta\)</span>.</p>
</div>
<p><strong>Exercise</strong> Show that <span class="math inline">\(MSE =  \text{Variance} + \text{Bias}^2\)</span>.</p>
<details>
<summary>
Solution
</summary>
<span class="math display">\[
\begin{align*}
\mathrm{MSE}(\hat\theta)
&amp;= E\big[(\hat\theta - \theta)^2\big] \\
&amp;= E\big(\hat\theta ^2\big) -2 \theta E\big(\hat\theta\big) + \theta^2\\
&amp;= \mathrm{Var}\big(\hat\theta\big) +  \big[E\big(\hat\theta\big)\big]^2 -2 \theta E\big(\hat\theta\big) + \theta^2\\
&amp;= \mathrm{Var}\big(\hat\theta\big) + \big[E\big(\hat\theta\big) - \theta\big]^2\\
&amp;= \mathrm{Var}(\hat\theta) + \big[\mathrm{Bias}(\hat\theta)\big]^2.
\end{align*}
\]</span>
</details>
<p>Here is a figure from Lohr’s text that shows the difference between low bias, low variance, and low MSE:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lec-2-images/lohr-precise-accurate.png" class="img-fluid figure-img" style="width:70.0%" alt="A figure from a text by Lohr showing the difference between accuracy, unbiasedness, and precision"></p>
<figcaption>Unbiased archers, precise archers, and accurate archers</figcaption>
</figure>
</div>
<p>This diagram shows that an estimator <span class="math inline">\(\hat{\theta}\)</span> is <strong>unbiased</strong> if <span class="math inline">\(E\big(\hat{\theta}\big) = \theta\)</span>, it is <strong>precise</strong> if <span class="math inline">\(\mathrm{Var}(\hat{\theta})\)</span> is small, but for the estimator to be <strong>accurate</strong>, <em>both</em> these quantities must be small, and therefore the Mean Squared Error (the sum of the squared bias and the variance) must be small, where <span class="math inline">\(MSE = E\big[\big(\hat{\theta} - \theta\big)^2\big]\)</span>.</p>
</section>
<section id="mathrmvaroverlinex-and-the-finite-population-correction" class="level3">
<h3 class="anchored" data-anchor-id="mathrmvaroverlinex-and-the-finite-population-correction"><span class="math inline">\(\mathrm{Var}(\overline{X})\)</span> and the finite population correction</h3>
<p>Recall that if we let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> are independent and identically distributed random variables (IID), with common expected value <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>; and <span class="math inline">\(\overline{X}\)</span> is the sample mean of this sample <span class="math inline">\(\big(\displaystyle \overline{X} = \dfrac{1}{n} \sum_{i=1}^n X_i\big)\)</span>, then <span class="math inline">\(E(\overline{X}) = \mu\)</span> and <span class="math inline">\(\mathrm{Var}(\overline{X}) = \sigma^2/n\)</span>. (<em>Note</em>: You should be able to show this.)</p>
<p>Now suppose we have a finite population of size <span class="math inline">\(N\)</span>, and we take a <strong>simple random sample</strong> of size <span class="math inline">\(n\)</span> from this population: <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>. Now the <span class="math inline">\(X_i\)</span> cannot be IID as we are sampling <em>without</em> replacement. It is easily shown (<em>Theorem A</em> on page 206) that the expected value of the sample mean is still <span class="math inline">\(\mu\)</span>, where <span class="math inline">\(\mu\)</span> is the population mean. What about <span class="math inline">\(\mathrm{Var}(\overline{X})\)</span>?</p>
It turns out that ( <em>Theorem B</em> on page 208): <span class="math display">\[
\mathrm{Var}(\overline{X}) = \dfrac{\sigma^2}{n}\left( \dfrac{N-n}{N-1}\right).
\]</span>
<details>
<summary>
Click for the proof
</summary>
<p><span class="math display">\[
\begin{align*}
\mathrm{Var}(\overline{X}) &amp;= \mathrm{Var}\left(\dfrac{1}{n}\sum_{i = 1}^n X_i\right)\\
  &amp;= \dfrac{1}{n^2} \mathrm{Var}\left(\sum_{i = 1}^n X_i\right), \: \text{because }\mathrm{Var}(aX) = a^2\mathrm{Var}(X)\\
  &amp;= \dfrac{1}{n^2}\mathrm{Cov}\left(\sum_{i = 1}^n X_i,\sum_{j = 1}^n X_j \right), \: \text{because } \mathrm{Var}(X) = \mathrm{Cov}(X,X)\\
  &amp;= \dfrac{1}{n^2}\left(\sum_{i = 1}^n \mathrm{Var}(X_i) + \sum_{i = 1}^n \sum_{\substack{j=1 \\ j \ne i}}^n \mathrm{Cov}(X_i,X_j)\right) \\
  &amp;= \dfrac{1}{n^2} \left( n\sigma^2 + n(n-1) \mathrm{Cov}(X_1, X_2) \right), \: \text{since all } n(n-1) \text{ pairs will have the same covariance.}
\end{align*}
\]</span> This computation implies that if we figure out <span class="math inline">\(\mathrm{Cov}(X_1, X_2)\)</span>, we will able to figure out the variance we need. So let’s compute this covariance. Recall that <span class="math inline">\(\mathrm{Cov}(X_1, X_2) = E(X_1 X_2) - E(X_1)E(X_2)\)</span>.</p>
<p>We know that the possible values of the <span class="math inline">\(X_i\)</span> are the population values: <span class="math inline">\(x_1, x_2, \ldots, x_N\)</span>. But some of these could be repeated, which can mess up the probability computations. To simplify our computations, we will define new values <span class="math inline">\(u_1, u_2, \ldots, u_m\)</span> to be the <em>distinct</em> values in the population, <span class="math inline">\(m\)</span> the <em>number</em> of distinct values, and let <span class="math inline">\(n_i\)</span> be the number of times we see the value <span class="math inline">\(u_i\)</span>.</p>
<p>For example, suppose <span class="math inline">\(N = 6\)</span> and the population values are <span class="math inline">\(1, 1, 4, 4, 4, 7\)</span>. Then <span class="math inline">\(x_1 = 1 = x_2, x_3 = x_4 = x_5 = 4\)</span>, and <span class="math inline">\(x_6 = 7\)</span>. Using the <span class="math inline">\(u_i's\)</span>, we have <span class="math inline">\(u_1 = 1, u_2 = 4, u_3 = 7\)</span>, and <span class="math inline">\(m=3\)</span>. Further, <span class="math inline">\(n_1 = 2, n_2 = 3, n_3 = 1\)</span>.</p>
<p>Now, if <span class="math inline">\(X_i\)</span> is the <span class="math inline">\(i\)</span>th sample value drawn, then <span class="math inline">\(X_i\)</span> is a <em>discrete</em> random variable such that <span class="math inline">\(P(X_i = u_i) = \dfrac{n_i}{N}\)</span>. This is because there are <em>still</em> <span class="math inline">\(N\)</span> total units in the population, and we have just grouped them by value.</p>
<p>For example, using the numbers above, <span class="math inline">\(P(X_i = 4) = \dfrac{3}{6}\)</span>.</p>
<p>You can check that <span class="math inline">\(E(X_i) = \mu\)</span> and <span class="math inline">\(\mathrm{Var}(X_i) = \sigma^2\)</span>, using the fact that <span class="math inline">\(\displaystyle \sum_{j=1}^m u_j n_j =  \sum_{i=1}^N x_i\)</span>.</p>
<p>Now let’s compute <span class="math inline">\(\mathrm{Cov}(X_1, X_2) = E(X_1 X_2) - \mu^2\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
E(X_1 X_2) &amp;= \sum_{i=1}^m \sum_{j=1}^m u_i u_j P(X_1 = u_i, X_2 = u_j) \\
    &amp;= \sum_{i=1}^m \sum_{j=1}^m u_i u_j P(X_1 = u_i) P(X_2 = u_j \vert X_1 = u_i) \\
    &amp;= \sum_{i=1}^m u_i P(X_1 = u_i) \sum_{j=1}^m u_j P(X_2 = u_j \vert X_1 = u_i)\\
\end{align*}
\]</span> Now, as we discussed earlier, <span class="math inline">\(P(X_1 = u_i) = \dfrac{n_i}{N}\)</span>. But the <em>second</em> draw from the population, <span class="math inline">\(X_2\)</span> will depend on the first. <span class="math inline">\(P(X_2 = u_j \vert X_1 = u_i) = \dfrac{n_j}{N-1}\)</span> if <span class="math inline">\(j \ne i\)</span> and <span class="math inline">\(P(X_2 = u_j \vert X_1 = u_i) = \dfrac{n_i-1}{N-1}\)</span> if <span class="math inline">\(j = i\)</span>.</p>
<p>Thus, we can simplify the interior sum to: <span class="math display">\[
\begin{align*}
\sum_{j=1}^m u_j P(X_2 = u_j \vert X_1 = u_i) &amp;= \sum_{\substack{j=1 \\ j \ne i}}^m u_j \cdot \dfrac{n_j}{N-1} + u_i\cdot \dfrac{n_i-1}{N-1}\\
  &amp;= \sum_{\substack{j=1 \\ j \ne i}}^m u_j \cdot \dfrac{n_j}{N-1} + u_i\cdot \dfrac{n_i}{N-1} - u_i\cdot \dfrac{1}{N-1}\\
  &amp;= \sum_{j=1}^m  \dfrac{u_j n_j}{N-1} - \dfrac{u_i}{N-1}
\end{align*}  
\]</span> Back to <span class="math inline">\(E(X_1 X_2)\)</span>, noting that <span class="math inline">\(\displaystyle \sum_{i=1}^m u_i n_i = \sum_{k=1}^N x_k = \tau = N\mu\)</span>, that is, the sum total of all the population values, and also note that <span class="math inline">\(\displaystyle \sum_{i=1}^m u_i^2 n_i = \sum_{i=1}^N x_i^2 = N(\sigma^2 + \mu^2)\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
E(X_1 X_2) &amp;= \sum_{i=1}^m u_i \dfrac{n_i}{N}\left[ \sum_{j=1}^m  \dfrac{u_j n_j}{N-1} - \dfrac{u_i}{N-1}\right]\\
   &amp;= \dfrac{1}{N(N-1)}\left[ \left( \sum_{i=1}^m u_i n_i\right)\left( \sum_{j=1}^m u_j n_j\right) - \sum_{i=1}^m u_i^2 n_i\right]\\
   &amp;= \dfrac{1}{N(N-1)} \left[ \left(N\mu\right)^2 -\sum_{i=1}^m u_i^2 n_i\right]\\
   &amp;= \dfrac{1}{N(N-1)}\left(N^2 \mu^2 - N(\sigma^2 + \mu^2) \right)\\
   &amp;= \mu^2 -\dfrac{\sigma^2}{N-1}
\end{align*}
\]</span> This implies that: <span class="math display">\[
\begin{align*}
\mathrm{Cov}(X_1, X_2) &amp;= E(X_1X_2) - \mu^2\\
   &amp;= \mu^2 -\dfrac{\sigma^2}{N-1} - \mu^2\\
   &amp;= -\dfrac{\sigma^2}{N-1}
\end{align*}
\]</span></p>
Now we can put it all together: <span class="math display">\[
\begin{align*}
\mathrm{Var}(\overline{X}) &amp;= \dfrac{1}{n^2} \left( n\sigma^2 + n(n-1) \mathrm{Cov}(X_1, X_2) \right)\\
   &amp;= \dfrac{1}{n^2} \left( n\sigma^2 - n(n-1)\dfrac{\sigma^2}{N-1}\right)\\
   &amp;= \dfrac{\sigma^2}{n}\left(1- \dfrac{n-1}{N-1}\right)\\
   &amp;= \dfrac{\sigma^2}{n}\left(\dfrac{N-n}{N-1}\right)
\end{align*}
\]</span>
</details>
<section id="finite-population-correction" class="level4">
<h4 class="anchored" data-anchor-id="finite-population-correction">Finite population correction</h4>
<p>The quantity <span class="math inline">\(\displaystyle \left( \dfrac{N-n}{N-1}\right)=\left(1- \dfrac{n-1}{N-1}\right)\)</span> is called the <strong>finite population correction</strong>. Note that <span class="math inline">\(\displaystyle \dfrac{n-1}{N-1} \approx \dfrac{n}{N}\)</span>, which is called the <strong>sampling fraction</strong>. The larger the sampling fraction, the larger the sample relative to the population, which means we have more information about the population. This should reduce the variability. The extreme case is when <span class="math inline">\(n=N\)</span>, and the sample mean has <em>no</em> variability. In practice, the sampling fraction is very small, and so the finite population correction is approximately 1. This means that the precision of the estimator (determined by the variance) depends only on the sample size, and not on the population size.</p>
</section>
</section>
<section id="estimating-the-population-variance" class="level3">
<h3 class="anchored" data-anchor-id="estimating-the-population-variance">Estimating the Population Variance</h3>
<p>We know that the population variance <span class="math inline">\(\sigma^2\)</span> is defined by: <span class="math display">\[
\sigma^2 = \dfrac{1}{N}\sum_{i=1}^N x_i^2 - \mu^2.
\]</span> We can define the quantity <span class="math inline">\(\hat{\sigma}^2\)</span>, which is a function of the sample <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>: <span class="math display">\[
\begin{align*}
\hat{\sigma}^2 &amp;= \dfrac{1}{n} \sum_{i=1}^n (X_i -\overline{X})^2 \\
    &amp;= \dfrac{1}{n} \sum_{i=1}^n X_i^2 -\overline{X}^2 \\
\end{align*}
\]</span> and use this to estimate <span class="math inline">\(\sigma^2\)</span>. The question is then if this estimator is <strong>unbiased</strong>. Is <span class="math inline">\(E(\hat{\sigma}^2) = \sigma^2\)</span>?</p>
<p><span class="math display">\[
\begin{align*}
E(\hat{\sigma}^2) &amp;= E\left( \dfrac{1}{n} \sum_{i=1}^n X_i^2 -\overline{X}^2 \right) \\
   &amp;= \dfrac{1}{n} \sum_{i=1}^n E\left(X_i^2\right) - E\big(\overline{X}^2 \big)\\
   &amp;=  (\sigma^2 + \mu^2) - E\big(\overline{X}^2 \big)\\
\end{align*}
\]</span> The last line is because <span class="math inline">\(\mathrm{Var}(X_i) = \sigma^2 = E(X_i^2) - \mu^2\)</span>. Doing a similar computation with <span class="math inline">\(E\big(\overline{X}^2 \big)\)</span>, we see that (for a simple random sample): <span class="math display">\[
E\big(\overline{X}^2 \big) = \mathrm{Var}(\overline{X}) + \mu^2 = \dfrac{\sigma^2}{n}\left(\dfrac{N-n}{N-1} \right) + \mu^2.
\]</span> Putting these together, and doing some tedious algebra offline, we have <span class="math display">\[
\begin{align*}
E(\hat{\sigma}^2) &amp;= (\sigma^2 + \mu^2) - \left[\frac{\sigma^2}{n}\left(\dfrac{N-n}{N-1} \right) + \mu^2\right] \\
   &amp;= \dfrac{n\sigma^2}{n} - \frac{\sigma^2}{n}\left(\dfrac{N-n}{N-1} \right) + \mu^2 - \mu^2  \\
   &amp;= \frac{\sigma^2}{n} \left[n - \left(\dfrac{N-n}{N-1} \right)\right] \\
   &amp;= \sigma^2 \left[\left(\frac{n-1}{n}\right) \left(\dfrac{N}{N-1} \right)\right] \\
   &amp;= \frac{\sigma^2}{n} \left[\dfrac{nN-N}{nN-1}\right]\\
\end{align*}
\]</span> This means that <span class="math inline">\(E(\hat{\sigma}^2) \ne \sigma^2\)</span>, and also that <span class="math inline">\(\hat{\sigma}^2\)</span> <strong>underestimates</strong> <span class="math inline">\(\sigma^2\)</span> on average (since <span class="math inline">\(N &gt; 1\)</span>). Therefore, to get an <strong>unbiased</strong> estimator of the variance of the sample mean, we need to multiply <span class="math inline">\(\hat{\sigma}^2\)</span> by the appropriate factor. Note that: <span class="math display">\[
E\left[\left(\frac{n}{n-1}\right) \left(\dfrac{N-1}{N} \right)\hat{\sigma}^2\right] = \sigma^2
\]</span> Our goal, of course, is to get an unbiased estimator for the variance of the sample mean. Recall that, for a simple random sample, <span class="math inline">\(Var(\overline{X}) = \dfrac{\sigma^2}{n}\left(\dfrac{N-n}{N-1}\right)\)</span>. Let’s substiute the unbiased estimator that we just derived above: <span class="math display">\[
\begin{align*}
\mathrm{(Estimated)\, Var}(\overline{X}) &amp;= \left(\frac{n}{n-1}\right) \left(\dfrac{N-1}{N} \right)\hat{\sigma}^2 \cdot \frac{1}{n} \left(\dfrac{N-n}{N-1}\right) \\
&amp;= \frac{\hat{\sigma}^2}{n-1}\left(\frac{N-n}{N}\right)\\
&amp;= \frac{s^2}{n}\left(1-\frac{n}{N}\right)
\end{align*}
\]</span> where <span class="math inline">\(\displaystyle s^2 = \dfrac{1}{n-1} \sum_{i=1}^n \big(X_i - \overline{X}\big)^2\)</span>, so that <span class="math inline">\(\displaystyle \dfrac{s^2}{n} = \dfrac{\hat{\sigma}^2}{n-1}\)</span>.</p>
<p>Thus we have that <span class="math inline">\(s_{\overline{X}}^2\)</span> is an <strong>unbiased estimator</strong> of <span class="math inline">\(\sigma_{\overline{X}}^2=\mathrm{Var}(\overline{X})\)</span>.</p>
<p>If the population is dichotomous, then the estimator becomes: <span class="math display">\[
\mathrm{Var}(\hat{p}) = s_{\hat{p}}^2 = \frac{s^2}{n} = \frac{\hat{p}(1-\hat{p})}{n-1}.
\]</span></p>
<p>Putting all this together gives us the table on page 214 of the text, reproduced here:</p>
<section id="summary-of-estimators" class="level4">
<h4 class="anchored" data-anchor-id="summary-of-estimators">Summary of estimators</h4>
<div class="widecols">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 8%">
<col style="width: 11%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Population Parameter</th>
<th style="text-align: left;">Estimator</th>
<th style="text-align: left;">Variance of Estimator (Square of Standard Error)</th>
<th style="text-align: left;">Estimated Variance of the Estimator <br> (Square of Estimated SE)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\mu\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\overline{X}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sigma_{\overline{X}}^2 = \displaystyle \dfrac{\sigma^2}{n}\left(\dfrac{N-n}{N-1}\right)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(s_{\overline{X}}^2  = \displaystyle \dfrac{s^2}{n}\left(1-\dfrac{n}{N}\right)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(p\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\hat{p}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sigma_{\hat{p}}^2 = \displaystyle \dfrac{p(1-p)}{n}\left(\dfrac{N-n}{N-1}\right)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(s_{\hat{p}}^2  = \displaystyle \dfrac{\hat{p}(1-\hat{p})}{n-1}\left(1-\dfrac{n}{N}\right)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\tau\)</span></td>
<td style="text-align: left;"><span class="math inline">\(T = N\overline{X}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sigma_{\tau}^2 = N^2\sigma_{\overline{X}}^2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(s_{\tau}^2 = N^2s_{\overline{X}}^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\sigma^2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\left(1-\dfrac{1}{N}\right)s^2\)</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="the-asymptotic-sampling-distribution-of-the-sample-mean" class="level3">
<h3 class="anchored" data-anchor-id="the-asymptotic-sampling-distribution-of-the-sample-mean">The (Asymptotic) Sampling Distribution of the Sample Mean</h3>
<section id="the-central-limit-theorem" class="level4">
<h4 class="anchored" data-anchor-id="the-central-limit-theorem">The Central Limit Theorem</h4>
<p>The CLT states that for large <span class="math inline">\(n\)</span>, the sample mean, suitably standardized, will have a CDF that approaches the CDF of the standard Normal. That is, the standardized sample mean <strong>converges in distribution</strong> to the standard Normal.</p>
<p>If <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> is an independent and identically distributed sample from a population with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then:</p>
<p><span class="math display">\[
\left(\frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\right) = \sqrt{n}\left(\frac{\overline{X}-\mu}{\sigma}\right) \overset{dsn}{\longrightarrow}\mathcal{N}(0,1)  \text{ as } n\longrightarrow \infty
\]</span> The CDF converges to <span class="math inline">\(\Phi\)</span>, which means that: <span class="math display">\[
P\left(\frac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}} \le z \right) = F_{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}}(z)\longrightarrow \Phi(z)
\]</span></p>
<p>The CLT is an incredibly important theorem, because it guarantees that for a large enough sample, <em>no matter what the distribution of the random variables <span class="math inline">\(X_i\)</span></em>, the sample mean behaves as though it is from an approximately <span class="math inline">\(\mathcal{N}(\mu, \dfrac{\sigma^2}{n})\)</span> distribution.</p>
<p>Note that the CLT is a <strong>limit</strong> theorem, so it fully characterizes the <em>asymptotic</em> distribution of the sample mean. It provides a good approximation for large enough samples, so we can compute probabilities such as the <span class="math inline">\(P\)</span>-value, and construct confidence intervals. We usually have a fixed population size <span class="math inline">\(N\)</span>, so it doesn’t make sense for the sample size <span class="math inline">\(n \rightarrow \infty\)</span>, but as long as <span class="math inline">\(n\)</span> is large, but the sampling fraction <span class="math inline">\(\dfrac{n}{N}\)</span> is small, the normal approximation is pretty good, and how we use it is demonstrated in the following example.</p>
</section>
<section id="example-a-page-201" class="level4">
<h4 class="anchored" data-anchor-id="example-a-page-201">Example A, page 201</h4>
<p>This is an example from the text that is used to illustrate many of the ideas from this chapter. Herkson (<em>JASA</em> 1976) presented data on the number of patients discharged from each of a population of <span class="math inline">\(N=393\)</span> hospitals during January 1986. The mean number of discharges across the population is about 815, and the population standard deviation is about 590.</p>
<ol type="a">
<li><p>If a random sample of <span class="math inline">\(n=100\)</span> is taken from this population <strong>with</strong> replacement, what is the <strong>standard error</strong> of the associated estimator of the population mean <span class="math inline">\(\overline{X}\)</span>?</p></li>
<li><p>What is the standard error of <span class="math inline">\(\overline{X}\)</span> if instead we take a <em>simple random sample</em> of size <span class="math inline">\(n=100\)</span>?</p></li>
<li><p>If we are sampling with replacement, what is the (approximate) probability that our sample average exceeds 850?</p></li>
</ol>
<details>
<summary>
Check your answer
</summary>
<ol type="a">
<li><p>Since we are sampling <em>with</em> replacement, the random variables form an IID sample, and so the standard error of the sample mean is <span class="math inline">\(\dfrac{\sigma}{\sqrt{n}} = \dfrac{590}{\sqrt{100}} = 59\)</span>.</p></li>
<li><p>In the case of an SRS, the standard error of the sample mean is given by <span class="math inline">\(\dfrac{\sigma}{\sqrt{n}}\left(\dfrac{N-n}{N-1}\right) = \dfrac{590}{\sqrt{100}}\sqrt{\left(\dfrac{393-100}{393-1}\right)} \approx 51\)</span>.</p></li>
<li><p>We want to approximate <span class="math inline">\(P(\overline{X} &gt; 850)\)</span>. By the CLT, <span class="math display">\[
P(\overline{X} &gt; 850) = P\left(\dfrac{\overline{X}-815}{59} &gt; \dfrac{850-815}{59}\right) = 1-\Phi\left(\dfrac{850-815}{59}\right) \approx 0.2765.
\]</span> We used <code>1-pnorm((850-815)/59)</code> to compute the answer.</p></li>
</ol>
</details>
</section>
</section>
<section id="confidence-intervals-for-the-population-mean" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals-for-the-population-mean">Confidence intervals for the Population Mean</h3>
<p>Another way we could use the central limit theorem with estimated standard error is to build a range of plausible values for the population mean from the sample. This range is called the confidence interval. A <strong>confidence interval </strong> for some population parameter <span class="math inline">\(\theta\)</span> is a <strong>random</strong> interval, whose endpoints are constructed using the sample, such that the interval contains <span class="math inline">\(\theta\)</span> with some specified probability.</p>
<p>It is very important to note the assumption that the population parameter is <strong>fixed</strong>, and it is the <em>interval</em> that is random, and so the probability of coverage is associated with the <strong>random interval</strong>.</p>
<p>Since we compute the endpoints using the random sample, the endpoints are random variables. This means that each time we take a sample of size <span class="math inline">\(n\)</span>, and then plug in our observed data, we will get a different <em>realization</em> of this random interval. But because we can use the CLT to approximate probabilities, we can construct intervals that have a probability of <span class="math inline">\(1-\alpha\)</span> of containing the true value. For example if <span class="math inline">\(\alpha = 0.05\)</span>, we construct an interval that contains the true mean 95% of the times (on average).</p>
<p>For <span class="math inline">\(0 \le \alpha \le 1\)</span>, let <span class="math inline">\(z(\alpha)\)</span> denote that value on the <span class="math inline">\(x\)</span>-axis such that the area under the standard normal density curve to the <strong>right</strong> of <span class="math inline">\(z(\alpha)\)</span> is <span class="math inline">\(\alpha\)</span>. When we have a <em>particular</em> confidence interval, it is a <em>realization</em> of a random interval, where the random interval has a certain coverage probability of <span class="math inline">\(1-\alpha\)</span>. We call this <em>coverage probability</em> the confidence level.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lecture-2_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480"></p>
</figure>
</div>
</div>
</div>
<p>Let’s derive the confidence interval for the population mean <span class="math inline">\(\mu\)</span>. By the central limit theorem, we know that <span class="math inline">\(\overline{X}\)</span> is approximately normal, that is, <span class="math inline">\(\dfrac{\overline{X}-\mu}{\sigma_{\overline{X}}} \approx \mathcal{N}(0,1)\)</span>.</p>
<p>If <span class="math inline">\(Z\)</span> follows the standard Normal distribution, then by the definition of <span class="math inline">\(z(\alpha)\)</span> above, we see that <span class="math display">\[
P\big(-z(\alpha/2) \le Z \le z(\alpha/2)\big) = 1-\alpha.
\]</span> Therefore, if <span class="math inline">\(\dfrac{\overline{X} - \mu}{\sigma/\sqrt{n}}\)</span> is approximately normal, we have that: <span class="math display">\[
P\left(-z(\alpha/2) \le \dfrac{\overline{X} - \mu}{\sigma/\sqrt{n}} \le z(\alpha/2)\right) \approx 1-\alpha.
\]</span> Now we multiply by <span class="math inline">\(\sigma/\sqrt{n}\)</span>, subtract <span class="math inline">\(\overline{X}\)</span> and multiply by <span class="math inline">\(-1\)</span>. This gives us the confidence interval that we need: <span class="math display">\[
P\left(\overline{X} - \frac{\sigma}{\sqrt{n}}z(\alpha/2) \le  \mu \le \overline{X} +  \frac{\sigma}{\sqrt{n}}z(\alpha/2)\right) \approx 1-\alpha.
\]</span> This statement says that the chance of this random interval, <span class="math inline">\(\left(\overline{X} - \dfrac{\sigma}{\sqrt{n}}z(\alpha/2) ,\; \overline{X} +  \dfrac{\sigma}{\sqrt{n}}z(\alpha/2)\right)\)</span> capturing the mean is approximately <span class="math inline">\(1-\alpha\)</span>, and so the interval is called a <strong><span class="math inline">\(100(1-\alpha)%\)</span> confidence interval</strong>.</p>
<p>We can then plug in our <em>observed</em> value of <span class="math inline">\(\overline{X}\)</span> and will get a <em>realization</em> of the random interval: <span class="math inline">\(\left(\overline{x} - \dfrac{\sigma}{\sqrt{n}}z(\alpha/2) ,\; \overline{x} +  \dfrac{\sigma}{\sqrt{n}}z(\alpha/2)\right).\)</span> Note that <em>this</em> interval is <strong>not</strong> random. It is just an interval on the real line and as <span class="math inline">\(\mu\)</span> is just some fixed constant, it either lies in this interval or does not. Therefore, once we plug in the observed sample mean, and the observed value of the estimator <span class="math inline">\(s_{\overline{X}}\)</span>, we don’t have any randomness. All the randomness is in the <em>sampling procedure</em>.</p>
<p>We can use the confidence interval to plan our data collection. Since the width of the confidence interval is given by <span class="math inline">\(2\times \dfrac{\sigma}{\sqrt{n}} \times z(\alpha)\)</span>, it is determined by <span class="math inline">\(\sigma\)</span> and by <span class="math inline">\(\sqrt{n}\)</span>. Now <span class="math inline">\(\sigma\)</span> is a constant of the population, so we can’t do much with it, but we can choose <span class="math inline">\(n\)</span> so that our confidence interval is as narrow as we desire. The <em>margin of error</em> of the confidence interval is given by $ z().$</p>
<p><strong>Exercise</strong> (Problem 8 from section 7.7) A sample of size 100 is taken from a population that has a proportion <span class="math inline">\(p = 1/5\)</span>.</p>
<ol type="a">
<li><p>Find <span class="math inline">\(\delta\)</span> such that <span class="math inline">\(P\big(\lvert \hat{p}-p\rvert \ge \delta\big)=0.025\)</span></p></li>
<li><p>If, in the sample, <span class="math inline">\(\hat{p} = 0.25\)</span>, will the 95% confidence interval for <span class="math inline">\(p\)</span> contain the true value for <span class="math inline">\(p\)</span>?</p></li>
</ol>
<details>
<summary>
Check your answer
</summary>
<ol type="a">
<li><span class="math inline">\(\delta = 0.0896\)</span>, <span class="math inline">\(p\)</span> is given to be <span class="math inline">\(\dfrac{1}{5}\)</span>. Therefore, <span class="math inline">\(\sigma_{\hat{p}} = \displaystyle \sqrt{\dfrac{\frac{1}{5}\cdot \frac{4}{5}}{100}} = \frac{2}{50}.\)</span></li>
</ol>
<p>By the Central Limit Theorem, <span class="math inline">\(\hat{p}\)</span> is approximately <span class="math inline">\(\mathcal{N}(p, \sigma_{\hat{p}}^2)\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
P\big(\lvert \hat{p}-p\rvert \ge \delta\big) &amp;= 0.025 \\
\Rightarrow P\big(\lvert \hat{p}-p\rvert &lt; \delta\big) &amp;= 0.975 \\
\Rightarrow P(-\delta &lt; \hat{p}-p &lt; \delta)  &amp;= 0.975 \\
\Rightarrow P\left(-\frac{\delta}{\sigma_{\hat{p}}} &lt; \frac{\hat{p}-p}{\sigma_{\hat{p}}} &lt; \frac{\delta}{\sigma_{\hat{p}}}\right)  &amp;= 0.975 \\
\Rightarrow P\left(-\frac{\delta}{\sigma_{\hat{p}}} &lt; Z &lt; \frac{\delta}{\sigma_{\hat{p}}}\right)  &amp;\approx 0.975 \\
\Rightarrow \Phi\left(\frac{\delta}{\sigma_{\hat{p}}}\right) - \Phi\left(-\frac{\delta}{\sigma_{\hat{p}}}\right) &amp;\approx 0.975 \\
\Rightarrow 2\Phi\left(\frac{\delta}{\sigma_{\hat{p}}}\right) -1 &amp;\approx 0.975 \\
\Rightarrow \Phi\left(\frac{\delta}{\sigma_{\hat{p}}}\right) &amp;\approx 0.9875 \\
\Rightarrow \frac{\delta}{\sigma_{\hat{p}}} &amp;\approx 2.24\\
\end{align*}
\]</span> Where we used <code>qnorm(0.9875)</code> to obtain 2.24. Plugging in the value of <span class="math inline">\(\sigma_{\hat{p}} = \dfrac{2}{50}\)</span>, we get that <span class="math inline">\(\delta\)</span> is about <span class="math inline">\(0.0896\)</span>.</p>
<ol start="2" type="a">
<li>Yes.</li>
</ol>
<p><span class="math inline">\(z(\alpha/2) = 1.96\)</span>, and the 95% confidence interval is given by <span class="math inline">\(\hat{p} \pm 1.96\times \dfrac{2}{50}\)</span>. Since <span class="math inline">\(\hat{p} = 0.25\)</span>, this gives us <span class="math inline">\(0.25 \pm 1.96\times \dfrac{2}{50} = (0.1716, 0.3284)\)</span> which contains <span class="math inline">\(p = \dfrac{1}{5}.\)</span></p>
</details>
<p><strong>Exercise</strong> 20 different polling companies have conducted independent surveys to estimate the proportion of US voters who approve of RFK Jr’s stewardship of Health and Human Services. Each company estimates this proportion using a 95% confidence interval. About how many do you think will be successful in covering the true proportion?</p>
<details>
<summary>
Check your answer
</summary>
<p>If we let <span class="math inline">\(Y\)</span> be the number of confidence intervals out of 20 that are successful, then since each interval has a 0.95 chance of success, we see that <span class="math inline">\(Y\sim Bin(20, 0.95)\)</span>. Therefore the expected number of successful intervals is <span class="math inline">\(E(Y) = 20\times 0.95 = 19.\)</span></p>
</details>
<div style="display:none">
<p><span class="citation" data-cites="rice2006 wasserman2004 samslides135 lohr2010">(<a href="#ref-rice2006" role="doc-biblioref">Rice 2006</a>; <a href="#ref-wasserman2004" role="doc-biblioref">Wasserman 2004</a>; <a href="#ref-samslides135" role="doc-biblioref">Pimentel 2024</a>; <a href="#ref-lohr2010" role="doc-biblioref">Lohr 2010</a>)</span></p>
</div>
</section>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-lohr2010" class="csl-entry" role="listitem">
Lohr, Sharon L. 2010. <em>Sampling: Design and Analysis</em>. 2nd ed. Cengage.
</div>
<div id="ref-samslides135" class="csl-entry" role="listitem">
Pimentel, Sam. 2024. <span>“STAT 135 Lecture Slides.”</span> Lecture slides (shared privately).
</div>
<div id="ref-rice2006" class="csl-entry" role="listitem">
Rice, John A. 2006. <em>Mathematical Statistics and Data Analysis</em>. 3rd ed. Duxbury Press.
</div>
<div id="ref-wasserman2004" class="csl-entry" role="listitem">
Wasserman, Larry. 2004. <em>All of Statistics: A Concise Course in Statistical Inference</em>. New York: Springer.
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/stat135\.berkeley\.edu\/spring-2026\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>